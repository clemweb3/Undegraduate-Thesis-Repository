{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4fa17b3",
   "metadata": {},
   "source": [
    "### 11. RFVI Construction: From Households to Regions\n",
    "\n",
    "**Objective:**\n",
    "To transform raw survey data into a single, comparable numberâ€”the **Regional Financial Vulnerability Index (RFVI)**.\n",
    "\n",
    "**The Logic (Plain English):**\n",
    "1.  **Scoring Households:** Imagine a household with 8 kids and no job. We use the weights from *Factor Analysis* (Notebook 10) to give this household a \"Vulnerability Score.\"\n",
    "2.  **Aggregating to Regions:** We cannot analyze 4 million households individually. We group them by Region (e.g., \"all households in Bicol\") and calculate the **average** score. This creates a \"Representative Household Profile\" for that region.\n",
    "3.  **The Index:** We combine the three dimensions (Sensitivity, Resilience, Exposure) into one final index using the formula:\n",
    "    $$RFVI = \\frac{\\text{Sensitivity} + (1 - \\text{Resilience}) + \\text{Exposure}}{3}$$\n",
    "    *(Note: We invert Resilience because high resilience is good, meaning it lowers vulnerability.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9afddef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading FA Weights from: G:\\My Drive\\Labor Force Survey\\FA_Results\n",
      "Saving Index to: G:\\My Drive\\Labor Force Survey\\RFVI_Results\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# --- Load Central Config ---\n",
    "# We use the shared config to ensure paths are identical to previous steps\n",
    "with open(Path(\"./data/interim/config.json\")) as f:\n",
    "    cfg = json.load(f)\n",
    "\n",
    "BASE_PATH = Path(cfg[\"BASE_PATH\"])\n",
    "IMPUTED_ROOT = BASE_PATH / \"Imputed Data for Analysis\"\n",
    "FA_RESULTS   = BASE_PATH / \"FA_Results\"\n",
    "OUTPUT_INDEX = BASE_PATH / \"RFVI_Results\"\n",
    "\n",
    "OUTPUT_INDEX.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Reading FA Weights from: {FA_RESULTS}\")\n",
    "print(f\"Saving Index to: {OUTPUT_INDEX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aac5de",
   "metadata": {},
   "source": [
    "### Step 1: Loading Validated Variables\n",
    "**Why this matters:**\n",
    "We do not manually pick variables here. Instead, we \"ask\" the Factor Analysis results (Notebook 10) which variables were statistically valid.\n",
    "* **Safety Check:** If a variable was dropped in the previous step (e.g., due to low KMO), it will automatically be excluded here. This prevents bias and ensures we only use reliable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9074991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 Sensitivity Variables.\n",
      "Loaded 7 Resilience Variables.\n",
      "Loaded 4 Exposure Variables.\n"
     ]
    }
   ],
   "source": [
    "def load_dimension_metadata(dim_name):\n",
    "    \"\"\"\n",
    "    Reads the factor loadings from Notebook 10 to identify surviving variables.\n",
    "    Returns: A dictionary of {Variable: Weight}\n",
    "    \"\"\"\n",
    "    path = FA_RESULTS / dim_name / \"factor_loadings.csv\"\n",
    "    if not path.exists():\n",
    "        print(f\"[WARNING] No FA results found for {dim_name}. Assuming weights=0.\")\n",
    "        return {}\n",
    "    \n",
    "    # Read the CSV (Index = Variable Name, Col 0 = Factor Weight)\n",
    "    df = pd.read_csv(path, index_col=0)\n",
    "    \n",
    "    # We use the primary factor (Factor_1) as the weight\n",
    "    return df.iloc[:, 0].to_dict()\n",
    "\n",
    "# --- Load Weights ---\n",
    "sensitivity_weights = load_dimension_metadata(\"Sensitivity\")\n",
    "resilience_weights  = load_dimension_metadata(\"Resilience\")\n",
    "exposure_weights    = load_dimension_metadata(\"Exposure\")\n",
    "\n",
    "print(f\"Loaded {len(sensitivity_weights)} Sensitivity Variables.\")\n",
    "print(f\"Loaded {len(resilience_weights)} Resilience Variables.\")\n",
    "print(f\"Loaded {len(exposure_weights)} Exposure Variables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd21bae",
   "metadata": {},
   "source": [
    "### Step 2: Standardization (Z-Scores)\n",
    "**Why we do this:**\n",
    "Our data has different units. *Income* is in thousands, while *Household Size* is single digits (1-10). If we just added them, Income would overwhelm the calculation.\n",
    "* **The Fix:** We convert everything to **Z-Scores**. This puts all variables on the same scale (centered at 0). A score of +1.0 means \"Higher than average,\" and -1.0 means \"Lower than average.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ae29e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(df, weights_dict):\n",
    "    \"\"\"\n",
    "    Standardizes data and calculates the weighted sum based on FA loadings.\n",
    "    \"\"\"\n",
    "    # 1. Filter to valid variables only\n",
    "    valid_vars = [v for v in weights_dict.keys() if v in df.columns]\n",
    "    if not valid_vars:\n",
    "        return np.zeros(len(df))\n",
    "\n",
    "    # 2. Extract Data\n",
    "    X = df[valid_vars].copy()\n",
    "    \n",
    "    # 3. Handle Non-Numeric Safety Check\n",
    "    X = X.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "    \n",
    "    # 4. Standardize (Z-Score transformation)\n",
    "    # This ensures Age (0-100) and Income (0-1M) are comparable\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # 5. Calculate Score: Data * Weight\n",
    "    w = np.array([weights_dict[v] for v in valid_vars])\n",
    "    return np.dot(X_scaled, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad0ecef",
   "metadata": {},
   "source": [
    "### Step 3: Regional Aggregation\n",
    "**What happens here:**\n",
    "We calculated a score for every single household. Now, we group them by **Region** and take the **Mean (Average)**.\n",
    "* **Interpretation:** The resulting number represents the \"Average Vulnerability Level\" of that region for that specific year. This condenses millions of rows into a clean table of ~17 regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "252ca6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Year: 2024...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juanp\\AppData\\Local\\Temp\\ipykernel_1360\\1281797143.py:12: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  d = pd.read_csv(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Year: 2023...\n",
      "Processing Year: 2022...\n",
      "Processing Year: 2019...\n",
      "Processing Year: 2018...\n",
      "Aggregation Complete. Final shape: (118, 5)\n"
     ]
    }
   ],
   "source": [
    "regional_aggregates = []\n",
    "\n",
    "# Loop through all imputed years\n",
    "for year_dir in IMPUTED_ROOT.iterdir():\n",
    "    if not year_dir.is_dir(): continue\n",
    "    \n",
    "    print(f\"Processing Year: {year_dir.name}...\")\n",
    "    \n",
    "    # Load all CSVs for that year\n",
    "    year_dfs = []\n",
    "    for f in year_dir.glob(\"imputed_*.csv\"):\n",
    "        d = pd.read_csv(f)\n",
    "        # Normalize columns to match the weights dictionary\n",
    "        d.columns = [str(c).strip().lower().replace(\"\\xa0\", \" \").replace(\"-\", \" \").replace(\"_\", \" \") for c in d.columns]\n",
    "        year_dfs.append(d)\n",
    "        \n",
    "    if not year_dfs: continue\n",
    "    full_df = pd.concat(year_dfs, ignore_index=True)\n",
    "    \n",
    "    # --- CALCULATE HOUSEHOLD SCORES ---\n",
    "    full_df['S_Score'] = calculate_score(full_df, sensitivity_weights)\n",
    "    full_df['R_Score'] = calculate_score(full_df, resilience_weights)\n",
    "    full_df['E_Score'] = calculate_score(full_df, exposure_weights)\n",
    "    \n",
    "    # --- AGGREGATE TO REGION ---\n",
    "    # Group by Region -> Take Mean\n",
    "    grouped = full_df.groupby('region')[['S_Score', 'R_Score', 'E_Score']].mean().reset_index()\n",
    "    grouped['Year'] = year_dir.name\n",
    "    \n",
    "    regional_aggregates.append(grouped)\n",
    "\n",
    "# Combine all years into one final dataset\n",
    "final_df = pd.concat(regional_aggregates, ignore_index=True)\n",
    "print(f\"Aggregation Complete. Final shape: {final_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c47fe6",
   "metadata": {},
   "source": [
    "### Step 4: Normalization & Index Calculation\n",
    "**Why normalize?**\n",
    "Factor scores can be strange numbers like -1.5 or +2.3. To make the index intuitive (like a grade from 0 to 100%), we scale them using **Min-Max Normalization**.\n",
    "* **0.0** = Lowest in the dataset (Best)\n",
    "* **1.0** = Highest in the dataset (Worst)\n",
    "\n",
    "**The RFVI Formula:**\n",
    "`RFVI = (Sensitivity + (1 - Resilience) + Exposure) / 3`\n",
    "* We use **(1 - Resilience)** because Resilience is good. If a region has High Resilience (1.0), it should contribute **0** to vulnerability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e2a51f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index Construction Complete.\n",
      "Sample Results:\n",
      "   Year                                        region      RFVI\n",
      "0  2024  Autonomous Region in Muslim Mindanao  (ARMM)  0.100328\n",
      "1  2024   Autonomous Region in Muslim Mindanao (ARMM)  0.332981\n",
      "2  2024       Cordillera Administrative Region  (CAR)  0.111880\n",
      "3  2024        Cordillera Administrative Region (CAR)  0.354247\n",
      "4  2024                               MIMAROPA Region  0.115327\n"
     ]
    }
   ],
   "source": [
    "# 1. Normalize Components (0 to 1 scale)\n",
    "scaler = MinMaxScaler()\n",
    "cols = ['S_Score', 'R_Score', 'E_Score']\n",
    "norm_cols = ['Sensitivity_Norm', 'Resilience_Norm', 'Exposure_Norm']\n",
    "\n",
    "final_df[norm_cols] = scaler.fit_transform(final_df[cols])\n",
    "\n",
    "# 2. Invert Resilience (High Resilience = Low Vulnerability)\n",
    "final_df['Inv_Resilience'] = 1 - final_df['Resilience_Norm']\n",
    "\n",
    "# 3. Calculate Final RFVI (Simple Average)\n",
    "final_df['RFVI'] = (\n",
    "    final_df['Sensitivity_Norm'] + \n",
    "    final_df['Inv_Resilience'] + \n",
    "    final_df['Exposure_Norm']\n",
    ") / 3\n",
    "\n",
    "# 4. Save\n",
    "out_path = OUTPUT_INDEX / \"Regional_Financial_Vulnerability_Index.csv\"\n",
    "final_df.to_csv(out_path, index=False)\n",
    "\n",
    "print(\"Index Construction Complete.\")\n",
    "print(\"Sample Results:\")\n",
    "print(final_df[['Year', 'region', 'RFVI']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998f2536",
   "metadata": {},
   "source": [
    "### 5. Interpretation of Results\n",
    "\n",
    "The table above represents the finalized **Regional Financial Vulnerability Index (RFVI)**. Here is how to interpret the specific columns generated by the pipeline:\n",
    "\n",
    "#### **A. Raw Factor Scores (`S_Score`, `R_Score`, `E_Score`)**\n",
    "These are the aggregated Z-scores derived from the household microdata.\n",
    "* **Scale:** Centered around 0.\n",
    "* **Interpretation:**\n",
    "    * **Positive values (+):** The region is *above the national average* for this dimension.\n",
    "    * **Negative values (-):** The region is *below the national average*.\n",
    "    * *Note:* A negative `S_Score` (Sensitivity) is good (less sensitive), while a negative `R_Score` (Resilience) is bad (less resilient).\n",
    "\n",
    "#### **B. Normalized Scores (`_Norm` columns)**\n",
    "To make the index comparable, we scaled the raw scores from 0 to 1 based on the minimum and maximum values found across the entire dataset.\n",
    "* **0.0:** The lowest value observed in the dataset.\n",
    "* **1.0:** The highest value observed in the dataset.\n",
    "\n",
    "#### **C. The \"Inverse Resilience\" Logic (`Inv_Resilience`)**\n",
    "This is a critical transformation step in our methodology.\n",
    "* **Concept:** Resilience acts as a *shield*. High resilience reduces vulnerability.\n",
    "* **The Math:** `Inv_Resilience = 1 - Resilience_Norm`\n",
    "* **Result:**\n",
    "    * If a region has **High Resilience** (e.g., 0.9), its vulnerability contribution becomes **Low** (0.1).\n",
    "    * If a region has **Low Resilience** (e.g., 0.2), its vulnerability contribution becomes **High** (0.8).\n",
    "\n",
    "#### **D. The Final Index (`RFVI`)**\n",
    "This is the composite score used for ranking and clustering.\n",
    "* **Formula:** Average of `Sensitivity_Norm`, `Exposure_Norm`, and `Inv_Resilience`.\n",
    "* **Scale:** 0 to 1.\n",
    "    * **Closer to 0:** **Low Vulnerability** (The region is stable, resilient, and safe).\n",
    "    * **Closer to 1:** **High Vulnerability** (The region is highly sensitive, exposed, and lacks resilience)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
