{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "163cd362",
   "metadata": {},
   "source": [
    "### Imputation Rationale\n",
    "\n",
    "**Do not impute inconsistent/partial variables by default.** Only consider imputation if the variable is conceptually indispensable and FMI suggests the information can be credibly recovered (e.g., plausible MAR with auxiliary predictors).\n",
    "\n",
    "It’s not reasonable to impute inconsistent/partial variables without first considering FMI and context. Imputation is not a neutral operation; it encodes assumptions about the missingness mechanism, temporal comparability, and the meaning of the variable. If a variable is inconsistent across months/years, imputing it can fabricate continuity that wasn’t in the data, undermining factor analysis and comparability across regions and time.\n",
    "\n",
    "**Tier 1 — Consistent variables:**\n",
    "\n",
    "- Action: Eligible for imputation.\n",
    "- Rule: Use FMI to determine imputation intensity (light/cautious/advanced).\n",
    "- Justification: Stable measurement; imputation supports matrix completion for EFA.\n",
    "\n",
    "**Tier 2 — Partial variables (intermittent presence or minor coding drift):**\n",
    "\n",
    "- Action: Conditional imputation.\n",
    "- Rule: Impute only if FMI is moderate/high but MAR plausibility exists via auxiliary predictors, and coding is harmonized; otherwise flag for sensitivity analysis.\n",
    "- Justification: Limited comparability; treat as supporting evidence, not core FA inputs.\n",
    "\n",
    "**Tier 3 — Inconsistent variables (structural changes, major coding breaks):**\n",
    "\n",
    "- Action: Do not impute for FA.\n",
    "- Rule: Document and retain for diagnostics; consider future harmonization projects or use in qualitative context.\n",
    "\n",
    "- Justification: Imputation would manufacture comparability and can distort factor structure.\n",
    "\n",
    "**Override - Conceptual indispensability:**\n",
    "\n",
    "- Action: If a variable is central to sensitivity/resilience/exposure and lacks a close proxy, allow imputation even if partial, but only with:\n",
    "- Explicit MAR argument using auxiliary variables,\n",
    "- complete coding evidence, and\n",
    "- Sensitivity analyses comparing included vs excluded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6116b65f",
   "metadata": {},
   "source": [
    "**Why imputing inconsistent variables without FMI review is not defensible?**\n",
    "\n",
    "Measurement instability:  \n",
    "\n",
    "Inconsistent variables often arise because the survey question changed, coding shifted, or the variable wasn’t asked in some rounds. Imputing them blindly assumes the missingness is random noise, when in fact it reflects structural differences. That creates false comparability across years.\n",
    "**Factor analysis assumptions:**\n",
    "\n",
    "FA assumes each variable measures the same construct across all observations. If a variable is inconsistent, imputing values fabricates continuity that wasn’t there. This risks producing spurious factors that look “interpretable” but are actually artifacts of imputation.\n",
    "\n",
    "**Auditability and thesis defense:**\n",
    "\n",
    "The approved pipeline methodology emphasizes transparency and conceptual justification. If the team imputes inconsistent variables without FMI, reviewers can easily challenge: “Why did you treat structurally missing data as if it were random?”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05331405",
   "metadata": {},
   "source": [
    "### Documentation and audit trail\n",
    "\n",
    "Action matrix: For each variable, store:\n",
    "\n",
    "- Tag: consistent/partial/inconsistent.\n",
    "- FMI bucket: Low/Moderate/High/Critical.\n",
    "- Dimension role: sensitivity/resilience/exposure.\n",
    "- Decision: keep, impute (light/cautious/advanced), sensitivity-only, exclude from FA.\n",
    "- Rationale: conceptual indispensability, MAR plausibility, harmonization status, auxiliary predictors.\n",
    "- Sensitivity analysis flags: Flag variables where inclusion materially changes factor loadings or KMO/Bartlett results, so the team can revisit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d0fd194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Decision matrix template saved to G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Decision Matrix for Imputation\\Decision_Matrix.csv\n"
     ]
    }
   ],
   "source": [
    "# 09_Imputation Notebook — Decision Matrix Builder\n",
    "# ------------------------------------------------\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Load config ---\n",
    "with open(Path(\"./data/interim/config.json\")) as f:\n",
    "    cfg = json.load(f)\n",
    "\n",
    "BASE_PATH = Path(cfg[\"BASE_PATH\"])\n",
    "INTERIM_DIR = Path(cfg[\"INTERIM_DIR\"])\n",
    "PROCESSED_DIR = Path(cfg[\"PROCESSED_DIR\"])\n",
    "LOG_DIR = Path(cfg[\"LOG_DIR\"])\n",
    "MONTH_ORDER = cfg[\"MONTH_ORDER\"]\n",
    "\n",
    "# --- Load inventory (optional, for parity) ---\n",
    "with open(Path(INTERIM_DIR) / \"inventory.json\") as f:\n",
    "    inventory = json.load(f)\n",
    "\n",
    "# --- Paths ---\n",
    "RENAMED_ROOT = BASE_PATH / \"NEW Renamed Fully Decoded Surveys\"\n",
    "CONSISTENCY_ROOT = BASE_PATH / \"NEW Variable Consistency Check\"\n",
    "FMI_ROOT = BASE_PATH / \"NEW FMI Reports\"\n",
    "DECISION_ROOT = BASE_PATH / \"Decision Matrix for Imputation\"\n",
    "os.makedirs(DECISION_ROOT, exist_ok=True)\n",
    "\n",
    "# --- Load inputs ---\n",
    "consistency_df = pd.read_csv(CONSISTENCY_ROOT / \"consistency_profile.csv\")\n",
    "fmi_df = pd.read_csv(FMI_ROOT / \"fmi_profile.csv\")\n",
    "\n",
    "# --- Merge consistency + FMI ---\n",
    "decision_df = fmi_df.merge(\n",
    "    consistency_df[[\"Variable\", \"ConsistencyTag\"]],\n",
    "    on=\"Variable\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# --- Handle duplicate ConsistencyTag columns if present ---\n",
    "if \"ConsistencyTag_x\" in decision_df.columns and \"ConsistencyTag_y\" in decision_df.columns:\n",
    "    decision_df[\"ConsistencyTag\"] = decision_df[\"ConsistencyTag_x\"].combine_first(decision_df[\"ConsistencyTag_y\"])\n",
    "    decision_df.drop(columns=[\"ConsistencyTag_x\", \"ConsistencyTag_y\"], inplace=True)\n",
    "\n",
    "# --- Manual factor formation dictionary (customizable) ---\n",
    "dimension_map = {\n",
    "    # Sensitivity\n",
    "    \"Available for Work\": \"Sensitivity\",\n",
    "    \"C13-Major Occupation Group\": \"Sensitivity\",\n",
    "    \"C14-Primary Occupation\": \"Sensitivity\",\n",
    "    \"C15-Major Industry Group\": \"Sensitivity\",\n",
    "    \"C16-Kind of Business (Primary Occupation)\": \"Sensitivity\",\n",
    "    \"C24-Basis of Payment (Primary Occupation)\": \"Sensitivity\",\n",
    "    \"C25-Basic Pay per Day (Primary Occupation)\": \"Sensitivity\",\n",
    "    \"Class of Worker (Primary Occupation)\": \"Sensitivity\",\n",
    "    \"Nature of Employment (Primary Occupation)\": \"Sensitivity\",\n",
    "    \"Total Hours Worked for all Jobs\": \"Sensitivity\",\n",
    "    \"Work Arrangement\": \"Sensitivity\",\n",
    "    \"Work Indicator\": \"Sensitivity\",\n",
    "    # Resilience\n",
    "    \"C03-Relationship to Household Head\": \"Resilience\",\n",
    "    \"C04-Sex\": \"Resilience\",\n",
    "    \"C05-Age as of Last Birthday\": \"Resilience\",\n",
    "    \"C06-Marital Status\": \"Resilience\",\n",
    "    \"C07-Highest Grade Completed\": \"Resilience\",\n",
    "    \"C08-Currently Attending School\": \"Resilience\",\n",
    "    \"C09-Graduate of technical/vocational course\": \"Resilience\",\n",
    "    \"C09a - Currently Attending Non-formal Training for Skills Development\": \"Resilience\",\n",
    "    \"Household Size\": \"Resilience\",\n",
    "    # Exposure\n",
    "    \"Province\": \"Exposure\",\n",
    "    \"Province Recode\": \"Exposure\",\n",
    "    \"Region\": \"Exposure\",\n",
    "    \"Urban-RuralFIES\": \"Exposure\",\n",
    "    \"Location of Work (Province, Municipality)\": \"Exposure\",\n",
    "    \"Survey Month\": \"Exposure\",\n",
    "    \"Survey Year\": \"Exposure\",\n",
    "}\n",
    "\n",
    "# --- Dimension assignment function ---\n",
    "def assign_dimension(var):\n",
    "    if var in dimension_map:\n",
    "        return dimension_map[var]\n",
    "    v = var.lower()\n",
    "    if any(k in v for k in [\"occupation\", \"work\", \"employment\", \"job\", \"hours\", \"basis\", \"industry\"]):\n",
    "        return \"Sensitivity\"\n",
    "    elif any(k in v for k in [\"grade\", \"school\", \"household\", \"age\", \"marital\", \"ethnicity\", \"training\"]):\n",
    "        return \"Resilience\"\n",
    "    elif any(k in v for k in [\"region\", \"province\", \"urban\", \"survey\", \"weight\", \"psu\", \"replicate\"]):\n",
    "        return \"Exposure\"\n",
    "    else:\n",
    "        return \"Unclassified\"\n",
    "\n",
    "decision_df[\"Dimension\"] = decision_df[\"Variable\"].apply(assign_dimension)\n",
    "\n",
    "# --- SuggestedAction logic ---\n",
    "def suggest_action(row):\n",
    "    fmi = row[\"OverallFMI\"]\n",
    "    tag = row[\"ConsistencyTag\"]\n",
    "\n",
    "    if pd.isna(fmi):\n",
    "        return \"review\"\n",
    "    if tag == \"consistent\":\n",
    "        if fmi < 0.05: return \"keep\"\n",
    "        elif fmi < 0.20: return \"impute_light\"\n",
    "        elif fmi < 0.40: return \"impute_cautious\"\n",
    "        else: return \"consider_drop_or_advanced\"\n",
    "    elif tag == \"partial\":\n",
    "        if fmi < 0.20: return \"sensitivity_only\"\n",
    "        else: return \"exclude_from_FA\"\n",
    "    else:  # inconsistent\n",
    "        return \"exclude_from_FA\"\n",
    "\n",
    "decision_df[\"Action\"] = decision_df.apply(suggest_action, axis=1)\n",
    "\n",
    "# --- Reorder columns for clarity ---\n",
    "decision_df = decision_df[[\n",
    "    \"Variable\", \"ConsistencyTag\", \"OverallFMI\", \"Flag\",\n",
    "    \"Dimension\", \"Action\", \n",
    "]]\n",
    "\n",
    "# --- Save template ---\n",
    "out_file = DECISION_ROOT / \"Decision_Matrix.csv\"\n",
    "decision_df.to_csv(out_file, index=False)\n",
    "print(f\"[OK] Decision matrix template saved to {out_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "510a30fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>ConsistencyTag</th>\n",
       "      <th>OverallFMI</th>\n",
       "      <th>Flag</th>\n",
       "      <th>Dimension</th>\n",
       "      <th>Action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Available for Work</td>\n",
       "      <td>consistent</td>\n",
       "      <td>0.965370</td>\n",
       "      <td>Critical</td>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>consider_drop_or_advanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C03-Relationship to Household Head</td>\n",
       "      <td>consistent</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Low</td>\n",
       "      <td>Resilience</td>\n",
       "      <td>keep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C04-Sex</td>\n",
       "      <td>consistent</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Low</td>\n",
       "      <td>Resilience</td>\n",
       "      <td>keep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C05-Age as of Last Birthday</td>\n",
       "      <td>consistent</td>\n",
       "      <td>0.016952</td>\n",
       "      <td>Low</td>\n",
       "      <td>Resilience</td>\n",
       "      <td>keep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C05B - Ethnicity</td>\n",
       "      <td>inconsistent</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Low</td>\n",
       "      <td>Resilience</td>\n",
       "      <td>exclude_from_FA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>C06-Marital Status</td>\n",
       "      <td>consistent</td>\n",
       "      <td>0.073508</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>Resilience</td>\n",
       "      <td>impute_light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>C07-Highest Grade Completed</td>\n",
       "      <td>consistent</td>\n",
       "      <td>0.074142</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>Resilience</td>\n",
       "      <td>impute_light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>C08-Currently Attending School</td>\n",
       "      <td>inconsistent</td>\n",
       "      <td>0.554300</td>\n",
       "      <td>Critical</td>\n",
       "      <td>Resilience</td>\n",
       "      <td>exclude_from_FA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>C09-Graduate of technical/vocational course</td>\n",
       "      <td>inconsistent</td>\n",
       "      <td>0.282487</td>\n",
       "      <td>High</td>\n",
       "      <td>Resilience</td>\n",
       "      <td>exclude_from_FA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>C09a - Currently Attending Non-formal Training...</td>\n",
       "      <td>inconsistent</td>\n",
       "      <td>0.279905</td>\n",
       "      <td>High</td>\n",
       "      <td>Resilience</td>\n",
       "      <td>exclude_from_FA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Variable ConsistencyTag  \\\n",
       "0                                 Available for Work     consistent   \n",
       "1                 C03-Relationship to Household Head     consistent   \n",
       "2                                            C04-Sex     consistent   \n",
       "3                        C05-Age as of Last Birthday     consistent   \n",
       "4                                   C05B - Ethnicity   inconsistent   \n",
       "5                                 C06-Marital Status     consistent   \n",
       "6                        C07-Highest Grade Completed     consistent   \n",
       "7                     C08-Currently Attending School   inconsistent   \n",
       "8        C09-Graduate of technical/vocational course   inconsistent   \n",
       "9  C09a - Currently Attending Non-formal Training...   inconsistent   \n",
       "\n",
       "   OverallFMI      Flag    Dimension                     Action  \n",
       "0    0.965370  Critical  Sensitivity  consider_drop_or_advanced  \n",
       "1    0.000000       Low   Resilience                       keep  \n",
       "2    0.000000       Low   Resilience                       keep  \n",
       "3    0.016952       Low   Resilience                       keep  \n",
       "4    0.000000       Low   Resilience            exclude_from_FA  \n",
       "5    0.073508  Moderate   Resilience               impute_light  \n",
       "6    0.074142  Moderate   Resilience               impute_light  \n",
       "7    0.554300  Critical   Resilience            exclude_from_FA  \n",
       "8    0.282487      High   Resilience            exclude_from_FA  \n",
       "9    0.279905      High   Resilience            exclude_from_FA  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa83e5fe",
   "metadata": {},
   "source": [
    "#### CRUCIAL NOTES (README)\n",
    "\n",
    "-  Not sure with the difference between `work indicator and work indicator.1.` Kindly see Decision_Matrix sheets for granular details.\n",
    "-  Also Check `Province and Province Recode` for missing values. Not sure what kind of imputation is applicable for this one since (assuming manual imputation, since lists of provinces can be acquired online and shall serve as a guide for encoding.). But we can still automate  this given that we have a strict list of dictionary once its acquired from online. IMPROPER IMPUTATION will done at this test stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255af06f",
   "metadata": {},
   "source": [
    "### Decision Matrix for Imputation - Defense\n",
    "\n",
    "This matrix is the bridge between FMI diagnostics and factor analysis.  \n",
    "It ensures that **every variable** is evaluated not only by its missingness (FMI) and consistency, but also by its **conceptual role** in financial vulnerability.\n",
    "\n",
    "- **Sensitivity**: Variables tied to employment stability, income regularity, and sectoral risk.  \n",
    "- **Resilience**: Variables reflecting household capacity, education, skills, and adaptability.  \n",
    "- **Exposure**: Variables representing structural or locational factors (region, province, urban/rural).\n",
    "\n",
    "#### Why automate?\n",
    "Manual factor formation was encoded into a reproducible dictionary and keyword rules.  \n",
    "This ensures consistency across runs, while still allowing customization:\n",
    "- The `dimension_map` dictionary can be edited to refine assignments.  \n",
    "- Keyword rules act as a fallback for variables not explicitly mapped.  \n",
    "- Any variable left as `\"Unclassified\"` is flagged for manual review.\n",
    "\n",
    "#### Why this is defensible?\n",
    "- **Theory-guided**: Dimensions are based on the approved thesis framework.  \n",
    "- **Transparent**: Every variable is listed, no silent exclusions.  \n",
    "- **Customizable**: Teammates can refine the dictionary or rationale column later.  \n",
    "- **Audit-ready**: The matrix documents not just FMI and consistency, but also conceptual relevance.\n",
    "\n",
    "This way, imputation decisions are **informed from the start**, but remain flexible for recalibration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b73f6b",
   "metadata": {},
   "source": [
    "### Imputation Proper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223895e3",
   "metadata": {},
   "source": [
    "At this stage, basic imputation will be done to the missing values following the mentioned criterias above. This notebook is customizable according to the further rules that will further be applied to the analysis. For further context, kindly read the CRUCIAL NOTES (README) section in this notebook outline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fa72926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing APRIL_2018.CSV from 2018\n",
      "[OK] Saved G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2018\\imputed_APRIL_2018.csv | Audit log: G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2018\\imputation_log_APRIL_2018.csv\n",
      "Processing JULY_2018.CSV from 2018\n",
      "[OK] Saved G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2018\\imputed_JULY_2018.csv | Audit log: G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2018\\imputation_log_JULY_2018.csv\n",
      "Processing JANUARY_2018.CSV from 2018\n",
      "[OK] Saved G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2018\\imputed_JANUARY_2018.csv | Audit log: G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2018\\imputation_log_JANUARY_2018.csv\n",
      "Processing OCTOBER_2018.CSV from 2018\n",
      "[OK] Saved G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2018\\imputed_OCTOBER_2018.csv | Audit log: G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2018\\imputation_log_OCTOBER_2018.csv\n",
      "Processing APRIL_2019.CSV from 2019\n",
      "[OK] Saved G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2019\\imputed_APRIL_2019.csv | Audit log: G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2019\\imputation_log_APRIL_2019.csv\n",
      "Processing JULY_2019.CSV from 2019\n",
      "[OK] Saved G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2019\\imputed_JULY_2019.csv | Audit log: G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2019\\imputation_log_JULY_2019.csv\n",
      "Processing OCTOBER_2019.CSV from 2019\n",
      "[OK] Saved G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2019\\imputed_OCTOBER_2019.csv | Audit log: G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2019\\imputation_log_OCTOBER_2019.csv\n",
      "Processing JANUARY_2019.CSV from 2019\n",
      "[OK] Saved G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2019\\imputed_JANUARY_2019.csv | Audit log: G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2019\\imputation_log_JANUARY_2019.csv\n",
      "Processing JULY_2022.CSV from 2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHANIA\\AppData\\Local\\Temp\\ipykernel_13100\\3896272969.py:181: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Saved G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2022\\imputed_JULY_2022.csv | Audit log: G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2022\\imputation_log_JULY_2022.csv\n",
      "Processing AUGUST_2022.CSV from 2022\n",
      "[OK] Saved G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2022\\imputed_AUGUST_2022.csv | Audit log: G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2022\\imputation_log_AUGUST_2022.csv\n",
      "Processing DECEMBER_2022.CSV from 2022\n",
      "[OK] Saved G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2022\\imputed_DECEMBER_2022.csv | Audit log: G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2022\\imputation_log_DECEMBER_2022.csv\n",
      "Processing NOVEMBER_2022.CSV from 2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHANIA\\AppData\\Local\\Temp\\ipykernel_13100\\3896272969.py:181: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Saved G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2022\\imputed_NOVEMBER_2022.csv | Audit log: G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2022\\imputation_log_NOVEMBER_2022.csv\n",
      "Processing OCTOBER_2022.CSV from 2022\n",
      "[OK] Saved G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2022\\imputed_OCTOBER_2022.csv | Audit log: G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2022\\imputation_log_OCTOBER_2022.csv\n",
      "Processing SEPTEMBER_2022.CSV from 2022\n",
      "[OK] Saved G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2022\\imputed_SEPTEMBER_2022.csv | Audit log: G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2022\\imputation_log_SEPTEMBER_2022.csv\n",
      "Processing APRIL_2023.CSV from 2023\n",
      "[OK] Saved G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2023\\imputed_APRIL_2023.csv | Audit log: G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2023\\imputation_log_APRIL_2023.csv\n",
      "Processing AUGUST_2023.CSV from 2023\n",
      "[OK] Saved G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2023\\imputed_AUGUST_2023.csv | Audit log: G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2023\\imputation_log_AUGUST_2023.csv\n",
      "Processing DECEMBER_2023.CSV from 2023\n",
      "[OK] Saved G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2023\\imputed_DECEMBER_2023.csv | Audit log: G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2023\\imputation_log_DECEMBER_2023.csv\n",
      "Processing FEBRUARY_2023.CSV from 2023\n",
      "[OK] Saved G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2023\\imputed_FEBRUARY_2023.csv | Audit log: G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2023\\imputation_log_FEBRUARY_2023.csv\n",
      "Processing JANUARY_2023.CSV from 2023\n",
      "[OK] Saved G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2023\\imputed_JANUARY_2023.csv | Audit log: G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2023\\imputation_log_JANUARY_2023.csv\n",
      "Processing JULY_2023.CSV from 2023\n",
      "[OK] Saved G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2023\\imputed_JULY_2023.csv | Audit log: G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2023\\imputation_log_JULY_2023.csv\n",
      "Processing JUNE_2023.CSV from 2023\n",
      "[OK] Saved G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2023\\imputed_JUNE_2023.csv | Audit log: G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2023\\imputation_log_JUNE_2023.csv\n",
      "Processing MARCH_2023.CSV from 2023\n",
      "[OK] Saved G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2023\\imputed_MARCH_2023.csv | Audit log: G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2023\\imputation_log_MARCH_2023.csv\n",
      "Processing NOVEMBER_2023.CSV from 2023\n",
      "[OK] Saved G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2023\\imputed_NOVEMBER_2023.csv | Audit log: G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2023\\imputation_log_NOVEMBER_2023.csv\n",
      "Processing OCTOBER_2023.CSV from 2023\n",
      "[OK] Saved G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2023\\imputed_OCTOBER_2023.csv | Audit log: G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2023\\imputation_log_OCTOBER_2023.csv\n",
      "Processing SEPTEMBER_2023.CSV from 2023\n",
      "[OK] Saved G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2023\\imputed_SEPTEMBER_2023.csv | Audit log: G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2023\\imputation_log_SEPTEMBER_2023.csv\n",
      "Processing MAY_2023.CSV from 2023\n",
      "[OK] Saved G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2023\\imputed_MAY_2023.csv | Audit log: G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2023\\imputation_log_MAY_2023.csv\n",
      "Processing FEBRUARY_2024.CSV from 2024\n",
      "[OK] Saved G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2024\\imputed_FEBRUARY_2024.csv | Audit log: G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2024\\imputation_log_FEBRUARY_2024.csv\n",
      "Processing APRIL_2024.CSV from 2024\n",
      "[OK] Saved G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2024\\imputed_APRIL_2024.csv | Audit log: G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2024\\imputation_log_APRIL_2024.csv\n",
      "Processing JANUARY_2024.CSV from 2024\n",
      "[OK] Saved G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2024\\imputed_JANUARY_2024.csv | Audit log: G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2024\\imputation_log_JANUARY_2024.csv\n",
      "Processing AUGUST_2024.CSV from 2024\n",
      "[OK] Saved G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2024\\imputed_AUGUST_2024.csv | Audit log: G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2024\\imputation_log_AUGUST_2024.csv\n",
      "Processing JULY_2024.CSV from 2024\n",
      "[OK] Saved G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2024\\imputed_JULY_2024.csv | Audit log: G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2024\\imputation_log_JULY_2024.csv\n",
      "Processing MARCH_2024.CSV from 2024\n",
      "[OK] Saved G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2024\\imputed_MARCH_2024.csv | Audit log: G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2024\\imputation_log_MARCH_2024.csv\n",
      "Processing MAY_2024.CSV from 2024\n",
      "[OK] Saved G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2024\\imputed_MAY_2024.csv | Audit log: G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2024\\imputation_log_MAY_2024.csv\n",
      "Processing JUNE_2024.CSV from 2024\n",
      "[OK] Saved G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2024\\imputed_JUNE_2024.csv | Audit log: G:\\.shortcut-targets-by-id\\1VctTphaltRx4xcPxmTJlRTrxLalyuEt8\\Labor Force Survey\\Imputed Data for Analysis\\2024\\imputation_log_JUNE_2024.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from difflib import get_close_matches\n",
    "\n",
    "# --- Paths ---\n",
    "INPUT_ROOT = BASE_PATH / \"NEW Renamed Fully Decoded Surveys\"\n",
    "CONSISTENCY_ROOT = BASE_PATH / \"NEW Variable Consistency Check\"\n",
    "FMI_ROOT = BASE_PATH / \"NEW FMI Reports\"\n",
    "OUTPUT_ROOT = BASE_PATH / \"Imputed Data for Analysis\"\n",
    "OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Load consistency + FMI profiles ---\n",
    "consistency_df = pd.read_csv(CONSISTENCY_ROOT / \"consistency_profile.csv\")\n",
    "fmi_df = pd.read_csv(FMI_ROOT / \"fmi_profile.csv\")\n",
    "\n",
    "decision_df = fmi_df.merge(\n",
    "    consistency_df[[\"Variable\", \"ConsistencyTag\"]],\n",
    "    on=\"Variable\", how=\"left\"\n",
    ")\n",
    "\n",
    "# Deduplicate merge artifacts\n",
    "if \"ConsistencyTag_x\" in decision_df.columns and \"ConsistencyTag_y\" in decision_df.columns:\n",
    "    decision_df[\"ConsistencyTag\"] = decision_df[\"ConsistencyTag_x\"].combine_first(decision_df[\"ConsistencyTag_y\"])\n",
    "    decision_df.drop(columns=[\"ConsistencyTag_x\", \"ConsistencyTag_y\"], inplace=True)\n",
    "\n",
    "# --- SuggestedAction logic ---\n",
    "def suggest_action(row):\n",
    "    fmi = row[\"OverallFMI\"]\n",
    "    tag = row[\"ConsistencyTag\"]\n",
    "    if pd.isna(fmi): return \"review\"\n",
    "    if tag == \"consistent\":\n",
    "        if fmi < 0.05: return \"keep\"\n",
    "        elif fmi < 0.20: return \"impute_light\"\n",
    "        elif fmi < 0.40: return \"impute_cautious\"\n",
    "        else: return \"consider_drop_or_advanced\"\n",
    "    elif tag == \"partial\":\n",
    "        if fmi < 0.20: return \"sensitivity_only\"\n",
    "        else: return \"exclude_from_FA\"\n",
    "    else:\n",
    "        return \"exclude_from_FA\"\n",
    "\n",
    "decision_df[\"Action\"] = decision_df.apply(suggest_action, axis=1)\n",
    "\n",
    "# --- Normalize names ---\n",
    "def normalize_name(name: str) -> str:\n",
    "    return (\n",
    "        str(name)\n",
    "        .strip()\n",
    "        .lower()\n",
    "        .replace(\"\\xa0\", \" \")\n",
    "        .replace(\"-\", \" \")\n",
    "        .replace(\"_\", \" \")\n",
    "    )\n",
    "\n",
    "decision_df[\"Variable_norm\"] = decision_df[\"Variable\"].apply(normalize_name)\n",
    "\n",
    "# --- Flexible finder with fuzzy matching ---\n",
    "def find_column(df, var):\n",
    "    cols_norm = {normalize_name(c): c for c in df.columns}\n",
    "    var_norm = normalize_name(var)\n",
    "\n",
    "    # exact match\n",
    "    if var_norm in cols_norm:\n",
    "        return cols_norm[var_norm]\n",
    "\n",
    "    # fuzzy match\n",
    "    matches = get_close_matches(var_norm, list(cols_norm.keys()), n=1, cutoff=0.8)\n",
    "    if matches:\n",
    "        return cols_norm[matches[0]]\n",
    "\n",
    "    return None\n",
    "\n",
    "# --- Helpers ---\n",
    "def robust_mode(series: pd.Series):\n",
    "    m = series.mode(dropna=True)\n",
    "    return None if m.empty else m.iloc[0]\n",
    "\n",
    "def clean_age_column(col: pd.Series) -> pd.Series:\n",
    "    s = col.astype(str)\n",
    "    s = s.where(~s.str.contains(r\"\\d{4}-\\d{2}-\\d{2}\", regex=True), \"UnknownAge\")\n",
    "    numeric_coerced = pd.to_numeric(s, errors=\"coerce\")\n",
    "    if numeric_coerced.notna().sum() >= (0.5 * len(s)):\n",
    "        return numeric_coerced.fillna(-1).astype(int)\n",
    "    else:\n",
    "        s = s.replace({\"nan\": \"UnknownAge\"})\n",
    "        return s\n",
    "\n",
    "def apply_imputation(df: pd.DataFrame, var: str, action: str, audit_rows: list):\n",
    "    col_name = find_column(df, var)\n",
    "    if col_name is None:\n",
    "        audit_rows.append({\n",
    "            \"Variable\": var,\n",
    "            \"Action\": action,\n",
    "            \"MethodApplied\": \"not_matched\",\n",
    "            \"BeforeMissing\": None,\n",
    "            \"AfterMissing\": None,\n",
    "            \"Note\": \"Variable not matched to any column (check naming).\"\n",
    "        })\n",
    "        return\n",
    "\n",
    "    before_missing = int(df[col_name].isna().sum())\n",
    "    dtype_numeric = pd.api.types.is_numeric_dtype(df[col_name])\n",
    "\n",
    "    if normalize_name(var) == normalize_name(\"C05-Age as of Last Birthday\"):\n",
    "        df[col_name] = clean_age_column(df[col_name])\n",
    "        dtype_numeric = pd.api.types.is_numeric_dtype(df[col_name])\n",
    "\n",
    "    method, note = \"none\", \"No missing data observed; no imputation required.\"\n",
    "    after_missing = before_missing\n",
    "\n",
    "    if action == \"keep\":\n",
    "        method = \"none\"\n",
    "        note = \"Left as-is per Decision Matrix.\"\n",
    "    elif action == \"impute_light\":\n",
    "        if dtype_numeric:\n",
    "            med = df[col_name].median()\n",
    "            df[col_name].fillna(med, inplace=True)\n",
    "            method = \"median\"\n",
    "            note = f\"Numeric light imputation with median={med:.4f}.\"\n",
    "        else:\n",
    "            mode_val = robust_mode(df[col_name])\n",
    "            if mode_val is not None:\n",
    "                df[col_name].fillna(mode_val, inplace=True)\n",
    "                method = \"mode\"\n",
    "                note = f\"Categorical light imputation with mode='{mode_val}'.\"\n",
    "            else:\n",
    "                df[col_name].fillna(\"Unknown\", inplace=True)\n",
    "                method = \"unknown_fallback\"\n",
    "                note = \"No valid mode; filled with 'Unknown'.\"\n",
    "        after_missing = int(df[col_name].isna().sum())\n",
    "    elif action == \"impute_cautious\":\n",
    "        if dtype_numeric:\n",
    "            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            if len(numeric_cols) >= 2:\n",
    "                imputer = KNNImputer(n_neighbors=5)\n",
    "                imputed_numeric = pd.DataFrame(\n",
    "                    imputer.fit_transform(df[numeric_cols]),\n",
    "                    columns=numeric_cols, index=df.index\n",
    "                )\n",
    "                df[col_name] = imputed_numeric[col_name]\n",
    "                method = \"knn_k5\"\n",
    "                note = \"Numeric cautious imputation (KNN, k=5).\"\n",
    "            else:\n",
    "                med = df[col_name].median()\n",
    "                df[col_name].fillna(med, inplace=True)\n",
    "                method = \"median_fallback\"\n",
    "                note = \"Insufficient predictors; median fallback.\"\n",
    "        else:\n",
    "            mode_val = robust_mode(df[col_name])\n",
    "            if mode_val is not None:\n",
    "                df[col_name].fillna(mode_val, inplace=True)\n",
    "                method = \"mode_cautious\"\n",
    "                note = f\"Categorical cautious imputation with mode='{mode_val}'.\"\n",
    "            else:\n",
    "                df[col_name].fillna(\"Unknown\", inplace=True)\n",
    "                method = \"unknown_fallback\"\n",
    "                note = \"No valid mode; filled with 'Unknown'.\"\n",
    "        after_missing = int(df[col_name].isna().sum())\n",
    "\n",
    "    audit_rows.append({\n",
    "        \"Variable\": var,\n",
    "        \"Action\": action,\n",
    "        \"MethodApplied\": method,\n",
    "        \"BeforeMissing\": before_missing,\n",
    "        \"AfterMissing\": after_missing,\n",
    "        \"Note\": note\n",
    "    })\n",
    "\n",
    "# --- Year-by-year execution ---\n",
    "for year_folder in INPUT_ROOT.iterdir():\n",
    "    if not year_folder.is_dir():\n",
    "        continue\n",
    "\n",
    "    year_out_dir = OUTPUT_ROOT / year_folder.name\n",
    "    year_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for file in year_folder.glob(\"*.csv\"):\n",
    "        print(f\"Processing {file.name} from {year_folder.name}\")\n",
    "        df = pd.read_csv(file)\n",
    "\n",
    "        # Normalize df columns\n",
    "        df.columns = [normalize_name(c) for c in df.columns]\n",
    "\n",
    "        # Audit log\n",
    "        audit_rows = []\n",
    "        for _, r in decision_df[decision_df[\"ConsistencyTag\"] == \"consistent\"].iterrows():\n",
    "            apply_imputation(df, r[\"Variable\"], r[\"Action\"], audit_rows)\n",
    "\n",
    "        # Save imputed dataset\n",
    "        out_file = year_out_dir / f\"imputed_{file.stem}.csv\"\n",
    "        df.to_csv(out_file, index=False)\n",
    "\n",
    "        # Save audit log\n",
    "        audit_df = pd.DataFrame(audit_rows)\n",
    "        audit_file = year_out_dir / f\"imputation_log_{file.stem}.csv\"\n",
    "        audit_df.to_csv(audit_file, index=False)\n",
    "\n",
    "        print(f\"[OK] Saved {out_file} | Audit log: {audit_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f2ca23",
   "metadata": {},
   "source": [
    "# Imputation Pipeline Documentation\n",
    "\n",
    "### Overview\n",
    "This pipeline processes survey data year-by-year from *NEW Renamed Fully Decoded Surveys* and produces imputed datasets in *Imputed Data for Analysis*. Each year has its own subfolder with:\n",
    "- `imputed_<monthyear>.csv`: the cleaned dataset\n",
    "- `imputation_log_<monthyear>.csv`: detailed audit of imputation actions\n",
    "\n",
    "### Scope\n",
    "- Only variables tagged **consistent** in the Decision Matrix are imputed **FOR NOW.**\n",
    "- Variables tagged **inconsistent** or **consider_drop_or_advanced** are excluded.\n",
    "- Province and Province Recode are excluded pending dictionary-based encoding.\n",
    "- Partial variables are excluded unless explicitly toggled.\n",
    "\n",
    "### Imputation Rules\n",
    "- **Numeric, low FMI (<0.20)** → Median imputation\n",
    "- **Numeric, moderate FMI (0.20–0.40)** → KNN imputation (k=5), median fallback if insufficient predictors\n",
    "- **Categorical, low FMI (<0.20)** → Mode imputation, fallback \"Unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6ce31c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
