{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65a8a0d8",
   "metadata": {},
   "source": [
    "\n",
    "This notebook decodes header survey datasets using metadata Sheet 1 and Sheet 2 definitions.  \n",
    "It renames raw variable codes into human‑readable descriptions, ensuring consistency across all survey files.\n",
    "\n",
    "#### Dependencies\n",
    "- Requires `00_Settings.ipynb` to be executed first (defines `inventory`, `base_path`, and global settings).\n",
    "- Requires reshaped metadata outputs from `01_Metadata_Sheet1_Reshaper.ipynb`.\n",
    "#### Outputs\n",
    "- Decoded survey CSVs saved into the folder: **NEW Header Encoded Surveys**\n",
    "- Formal translation reports per survey (coverage, untranslated codes).\n",
    "#### Notes\n",
    "- This notebook is functional and does not require reruns once executed.\n",
    "- Each notebook in the pipeline retains its function and respects assigned variables consistently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dd7523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load settings from config.json (produced by 00_Settings.ipynb)\n",
    "# ------------------------------------------------------------\n",
    "with open(Path(\"./data/interim/config.json\")) as f:\n",
    "    cfg = json.load(f)\n",
    "\n",
    "BASE_PATH = Path(cfg[\"BASE_PATH\"])\n",
    "INTERIM_DIR = Path(cfg[\"INTERIM_DIR\"])\n",
    "PROCESSED_DIR = Path(cfg[\"PROCESSED_DIR\"])\n",
    "LOG_DIR = Path(cfg[\"LOG_DIR\"])\n",
    "MONTH_ORDER = cfg[\"MONTH_ORDER\"]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load inventory (produced by 01_Inventory.ipynb)\n",
    "# ------------------------------------------------------------\n",
    "with open(Path(INTERIM_DIR) / \"inventory.json\") as f:\n",
    "    inventory = json.load(f)\n",
    "\n",
    "# Alias for compatibility\n",
    "base_path = str(BASE_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9899899e",
   "metadata": {},
   "source": [
    "### Loader Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0ee8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_dataset(year, month, filetype=\"survey\"):\n",
    "    \"\"\"\n",
    "    Locate and load a dataset file (CSV or Excel) from the global inventory.\n",
    "    Relies on 'inventory' and 'BASE_PATH' defined in 00_Settings.ipynb.\n",
    "    \"\"\"\n",
    "    if year not in inventory or month not in inventory[year]:\n",
    "        raise ValueError(f\"No records found in inventory for {month} {year}.\")\n",
    "\n",
    "    files = inventory[year][month]\n",
    "    found_file = next((f for f in files if f['filetype'] == filetype), None)\n",
    "    if not found_file:\n",
    "        raise FileNotFoundError(f\"No {filetype} file found for {month} {year}.\")\n",
    "\n",
    "    file_path = os.path.join(base_path, year, found_file['filename'])\n",
    "    return pd.read_csv(file_path, low_memory=False) if filetype == \"survey\" else pd.read_excel(file_path)\n",
    "\n",
    "\n",
    "def load_clean_sheet1(year, month):\n",
    "    \"\"\"\n",
    "    Load processed variable definitions (Sheet 1) from 'Metadata Sheet 1 CSV's'.\n",
    "    \"\"\"\n",
    "    folder_name = \"Metadata Sheet 1 CSV's\"\n",
    "    filename = f\"Sheet1_{month}_{year}.csv\"\n",
    "    file_path = os.path.join(base_path, folder_name, year, filename)\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Processed metadata file not found at {file_path}\")\n",
    "\n",
    "    return pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec306f33",
   "metadata": {},
   "source": [
    "### Header translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1f7fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_metadata_headers(survey_df, metadata_sheet1_df, year=\"Unknown\", month=\"Survey\"):\n",
    "    \"\"\"\n",
    "    Rename raw survey columns using metadata definitions.\n",
    "    Prints a translation report showing coverage and untranslated codes.\n",
    "    \"\"\"\n",
    "    metadata_sheet1_df['Variable'] = metadata_sheet1_df['Variable'].astype(str).str.strip()\n",
    "    metadata_sheet1_df['Description'] = metadata_sheet1_df['Description'].astype(str).str.strip()\n",
    "\n",
    "    header_map = dict(zip(metadata_sheet1_df['Variable'], metadata_sheet1_df['Description']))\n",
    "\n",
    "    original_cols = set(survey_df.columns)\n",
    "    translated_cols = original_cols.intersection(header_map.keys())\n",
    "    untranslated_cols = original_cols - header_map.keys()\n",
    "\n",
    "    renamed_df = survey_df.rename(columns=header_map)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"METADATA TRANSLATION REPORT: {month.upper()} {year}\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total Columns Detected:       {len(original_cols)}\")\n",
    "    print(f\"Successfully Decoded:         {len(translated_cols)}\")\n",
    "    print(f\"Remaining as Raw Codes:       {len(untranslated_cols)}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    if not untranslated_cols:\n",
    "        print(\"Status: SUCCESS (100% Metadata Coverage)\")\n",
    "    else:\n",
    "        print(\"Status: PARTIAL SUCCESS\")\n",
    "        print(\"Untranslated Codes:\", sorted(list(untranslated_cols)))\n",
    "\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    return renamed_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39173b29",
   "metadata": {},
   "source": [
    "### Batch Automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281f2098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch_header_translation(inventory, base_path):\n",
    "    \"\"\"\n",
    "    Apply header translation to all survey CSVs and save results\n",
    "    into 'NEW Header Encoded Surveys'.\n",
    "    \"\"\"\n",
    "    output_folder_name = \"NEW Header Encoded Surveys\"\n",
    "    output_base_path = os.path.join(base_path, output_folder_name)\n",
    "    os.makedirs(output_base_path, exist_ok=True)\n",
    "\n",
    "    print(\"================================================\")\n",
    "    print(\"STARTING BATCH HEADER TRANSLATION\")\n",
    "    print(f\"Output Directory: {output_base_path}\")\n",
    "    print(\"================================================\\n\")\n",
    "\n",
    "    success_count, skip_count, error_count = 0, 0, 0\n",
    "\n",
    "    for year in sorted(inventory.keys()):\n",
    "        year_output_path = os.path.join(output_base_path, year)\n",
    "        os.makedirs(year_output_path, exist_ok=True)\n",
    "\n",
    "        for month, files_list in inventory[year].items():\n",
    "            if month == \"Unmatched\": continue\n",
    "            print(f\"Processing: {month.upper()} {year}...\")\n",
    "\n",
    "            try:\n",
    "                survey_file_data = next((f for f in files_list if f['filetype'] == 'survey'), None)\n",
    "                if not survey_file_data:\n",
    "                    print(\"   [SKIP] No raw survey CSV found.\")\n",
    "                    skip_count += 1\n",
    "                    continue\n",
    "\n",
    "                raw_survey = load_dataset(year, month, \"survey\")\n",
    "                clean_metadata = load_clean_sheet1(year, month)\n",
    "                decoded_df = apply_metadata_headers(raw_survey, clean_metadata, year, month)\n",
    "\n",
    "                save_path = os.path.join(year_output_path, survey_file_data['filename'])\n",
    "                decoded_df.to_csv(save_path, index=False)\n",
    "                print(f\"   [OK] Saved File: {survey_file_data['filename']}\")\n",
    "                success_count += 1\n",
    "\n",
    "            except FileNotFoundError:\n",
    "                print(f\"   [SKIP] Missing Metadata Sheet 1 CSV for {month} {year}.\")\n",
    "                skip_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"   [ERROR] Failed to process: {e}\")\n",
    "                error_count += 1\n",
    "\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "    print(\"\\n================================================\")\n",
    "    print(\"BATCH PROCESS COMPLETE\")\n",
    "    print(f\"   Successful: {success_count}\")\n",
    "    print(f\"   Skipped:    {skip_count}\")\n",
    "    print(f\"   Errors:     {error_count}\")\n",
    "    print(\"================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7940e8",
   "metadata": {},
   "source": [
    "### Integrity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004fff35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_header_decoding_integrity(inventory, base_path):\n",
    "    \"\"\"\n",
    "    Checks if all raw survey columns have been successfully decoded\n",
    "    using metadata Sheet 1.\n",
    "    \n",
    "    Returns a DataFrame with:\n",
    "    Year | Month | Raw Headers Count | Decoded Headers Count | Integrity Status\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for year, months_data in inventory.items():\n",
    "        for month, files_list in months_data.items():\n",
    "            if month == \"Unmatched\": continue\n",
    "\n",
    "            try:\n",
    "                raw_df = load_dataset(year, month, \"survey\")\n",
    "                raw_headers = list(raw_df.columns)\n",
    "                raw_count = len(raw_headers)\n",
    "\n",
    "                meta_df = load_clean_sheet1(year, month)\n",
    "                meta_df['Variable'] = meta_df['Variable'].astype(str).str.strip()\n",
    "                meta_df['Description'] = meta_df['Description'].astype(str).str.strip()\n",
    "                header_map = dict(zip(meta_df['Variable'], meta_df['Description']))\n",
    "\n",
    "                decoded_count = sum(col in header_map for col in raw_headers)\n",
    "                status = \"PASS\" if raw_count == decoded_count else \"FAIL\"\n",
    "\n",
    "                results.append({\n",
    "                    \"Year\": year,\n",
    "                    \"Month\": month,\n",
    "                    \"Raw Headers Count\": raw_count,\n",
    "                    \"Decoded Headers Count\": decoded_count,\n",
    "                    \"Integrity Status\": status\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                results.append({\n",
    "                    \"Year\": year,\n",
    "                    \"Month\": month,\n",
    "                    \"Raw Headers Count\": \"ERROR\",\n",
    "                    \"Decoded Headers Count\": \"ERROR\",\n",
    "                    \"Integrity Status\": f\"FAIL ({e})\"\n",
    "                })\n",
    "\n",
    "    result_df = pd.DataFrame(results).sort_values([\"Year\", \"Month\"]).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\n===== HEADER DECODING INTEGRITY CHECK COMPLETE =====\")\n",
    "    total_failures = (result_df[\"Integrity Status\"] != \"PASS\").sum()\n",
    "    if total_failures == 0:\n",
    "        print(\"SUCCESS: All survey column headers have been fully decoded.\")\n",
    "    else:\n",
    "        print(f\"Completed with {total_failures} months failing integrity checks.\")\n",
    "    print(\"====================================================\\n\")\n",
    "\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7e38e8",
   "metadata": {},
   "source": [
    "### Execution Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3686ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if 'inventory' in locals() and 'base_path' in locals():\n",
    "        run_batch_header_translation(inventory, base_path)\n",
    "        integrity_df = verify_header_decoding_integrity(inventory, base_path)\n",
    "        display(integrity_df)\n",
    "    else:\n",
    "        print(\"Skipping execution: 'inventory' or 'base_path' not found in scope.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
