{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "163cd362",
   "metadata": {},
   "source": [
    "### Imputation Rationale\n",
    "\n",
    "**Do not impute inconsistent/partial variables by default.** Only consider imputation if the variable is conceptually indispensable and FMI suggests the information can be credibly recovered (e.g., plausible MAR with auxiliary predictors).\n",
    "\n",
    "It’s not reasonable to impute inconsistent/partial variables without first considering FMI and context. Imputation is not a neutral operation; it encodes assumptions about the missingness mechanism, temporal comparability, and the meaning of the variable. If a variable is inconsistent across months/years, imputing it can fabricate continuity that wasn’t in the data, undermining factor analysis and comparability across regions and time.\n",
    "\n",
    "**Tier 1 — Consistent variables:**\n",
    "\n",
    "- Action: Eligible for imputation.\n",
    "- Rule: Use FMI to determine imputation intensity (light/cautious/advanced).\n",
    "- Justification: Stable measurement; imputation supports matrix completion for EFA.\n",
    "\n",
    "**Tier 2 — Partial variables (intermittent presence or minor coding drift):**\n",
    "\n",
    "- Action: Conditional imputation.\n",
    "- Rule: Impute only if FMI is moderate/high but MAR plausibility exists via auxiliary predictors, and coding is harmonized; otherwise flag for sensitivity analysis.\n",
    "- Justification: Limited comparability; treat as supporting evidence, not core FA inputs.\n",
    "\n",
    "**Tier 3 — Inconsistent variables (structural changes, major coding breaks):**\n",
    "\n",
    "- Action: Do not impute for FA.\n",
    "- Rule: Document and retain for diagnostics; consider future harmonization projects or use in qualitative context.\n",
    "\n",
    "- Justification: Imputation would manufacture comparability and can distort factor structure.\n",
    "\n",
    "**Override - Conceptual indispensability:**\n",
    "\n",
    "- Action: If a variable is central to sensitivity/resilience/exposure and lacks a close proxy, allow imputation even if partial, but only with:\n",
    "- Explicit MAR argument using auxiliary variables,\n",
    "- complete coding evidence, and\n",
    "- Sensitivity analyses comparing included vs excluded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6116b65f",
   "metadata": {},
   "source": [
    "**Why imputing inconsistent variables without FMI review is not defensible?**\n",
    "\n",
    "Measurement instability:  \n",
    "\n",
    "Inconsistent variables often arise because the survey question changed, coding shifted, or the variable wasn’t asked in some rounds. Imputing them blindly assumes the missingness is random noise, when in fact it reflects structural differences. That creates false comparability across years.\n",
    "**Factor analysis assumptions:**\n",
    "\n",
    "FA assumes each variable measures the same construct across all observations. If a variable is inconsistent, imputing values fabricates continuity that wasn’t there. This risks producing spurious factors that look “interpretable” but are actually artifacts of imputation.\n",
    "\n",
    "**Auditability and thesis defense:**\n",
    "\n",
    "The approved pipeline methodology emphasizes transparency and conceptual justification. If the team imputes inconsistent variables without FMI, reviewers can easily challenge: “Why did you treat structurally missing data as if it were random?”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05331405",
   "metadata": {},
   "source": [
    "### Documentation and audit trail\n",
    "\n",
    "Action matrix: For each variable, store:\n",
    "\n",
    "- Tag: consistent/partial/inconsistent.\n",
    "- FMI bucket: Low/Moderate/High/Critical.\n",
    "- Dimension role: sensitivity/resilience/exposure.\n",
    "- Decision: keep, impute (light/cautious/advanced), sensitivity-only, exclude from FA.\n",
    "- Rationale: conceptual indispensability, MAR plausibility, harmonization status, auxiliary predictors.\n",
    "- Sensitivity analysis flags: Flag variables where inclusion materially changes factor loadings or KMO/Bartlett results, so the team can revisit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d0fd194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Decision matrix template saved to G:\\My Drive\\Labor Force Survey\\Decision Matrix for Imputation\\Decision_Matrix.csv\n"
     ]
    }
   ],
   "source": [
    "# 09_Imputation Notebook — Decision Matrix Builder\n",
    "# ------------------------------------------------\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Load config ---\n",
    "with open(Path(\"./data/interim/config.json\")) as f:\n",
    "    cfg = json.load(f)\n",
    "\n",
    "BASE_PATH = Path(cfg[\"BASE_PATH\"])\n",
    "INTERIM_DIR = Path(cfg[\"INTERIM_DIR\"])\n",
    "PROCESSED_DIR = Path(cfg[\"PROCESSED_DIR\"])\n",
    "LOG_DIR = Path(cfg[\"LOG_DIR\"])\n",
    "MONTH_ORDER = cfg[\"MONTH_ORDER\"]\n",
    "\n",
    "# --- Load inventory (optional, for parity) ---\n",
    "with open(Path(INTERIM_DIR) / \"inventory.json\") as f:\n",
    "    inventory = json.load(f)\n",
    "\n",
    "# --- Paths ---\n",
    "RENAMED_ROOT = BASE_PATH / \"NEW Renamed Fully Decoded Surveys\"\n",
    "CONSISTENCY_ROOT = BASE_PATH / \"NEW Variable Consistency Check\"\n",
    "FMI_ROOT = BASE_PATH / \"NEW FMI Reports\"\n",
    "DECISION_ROOT = BASE_PATH / \"Decision Matrix for Imputation\"\n",
    "os.makedirs(DECISION_ROOT, exist_ok=True)\n",
    "\n",
    "# --- Load inputs ---\n",
    "consistency_df = pd.read_csv(CONSISTENCY_ROOT / \"consistency_profile.csv\")\n",
    "fmi_df = pd.read_csv(FMI_ROOT / \"fmi_profile.csv\")\n",
    "\n",
    "# --- Merge consistency + FMI ---\n",
    "decision_df = fmi_df.merge(\n",
    "    consistency_df[[\"Variable\", \"ConsistencyTag\"]],\n",
    "    on=\"Variable\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# --- Handle duplicate ConsistencyTag columns if present ---\n",
    "if \"ConsistencyTag_x\" in decision_df.columns and \"ConsistencyTag_y\" in decision_df.columns:\n",
    "    decision_df[\"ConsistencyTag\"] = decision_df[\"ConsistencyTag_x\"].combine_first(decision_df[\"ConsistencyTag_y\"])\n",
    "    decision_df.drop(columns=[\"ConsistencyTag_x\", \"ConsistencyTag_y\"], inplace=True)\n",
    "\n",
    "# --- Manual factor formation dictionary (customizable) ---\n",
    "dimension_map = {\n",
    "    # Sensitivity\n",
    "    \"Available for Work\": \"Sensitivity\",\n",
    "    \"C13-Major Occupation Group\": \"Sensitivity\",\n",
    "    \"C14-Primary Occupation\": \"Sensitivity\",\n",
    "    \"C15-Major Industry Group\": \"Sensitivity\",\n",
    "    \"C16-Kind of Business (Primary Occupation)\": \"Sensitivity\",\n",
    "    \"C24-Basis of Payment (Primary Occupation)\": \"Sensitivity\",\n",
    "    \"C25-Basic Pay per Day (Primary Occupation)\": \"Sensitivity\",\n",
    "    \"Class of Worker (Primary Occupation)\": \"Sensitivity\",\n",
    "    \"Nature of Employment (Primary Occupation)\": \"Sensitivity\",\n",
    "    \"Total Hours Worked for all Jobs\": \"Sensitivity\",\n",
    "    \"Work Arrangement\": \"Sensitivity\",\n",
    "    \"Work Indicator\": \"Sensitivity\",\n",
    "    # Resilience\n",
    "    \"C03-Relationship to Household Head\": \"Resilience\",\n",
    "    \"C04-Sex\": \"Resilience\",\n",
    "    \"C05-Age as of Last Birthday\": \"Resilience\",\n",
    "    \"C06-Marital Status\": \"Resilience\",\n",
    "    \"C07-Highest Grade Completed\": \"Resilience\",\n",
    "    \"C08-Currently Attending School\": \"Resilience\",\n",
    "    \"C09-Graduate of technical/vocational course\": \"Resilience\",\n",
    "    \"C09a - Currently Attending Non-formal Training for Skills Development\": \"Resilience\",\n",
    "    \"Household Size\": \"Resilience\",\n",
    "    # Exposure\n",
    "    \"Province\": \"Exposure\",\n",
    "    \"Province Recode\": \"Exposure\",\n",
    "    \"Region\": \"Exposure\",\n",
    "    \"Urban-RuralFIES\": \"Exposure\",\n",
    "    \"Location of Work (Province, Municipality)\": \"Exposure\",\n",
    "    \"Survey Month\": \"Exposure\",\n",
    "    \"Survey Year\": \"Exposure\",\n",
    "}\n",
    "\n",
    "# --- Dimension assignment function ---\n",
    "def assign_dimension(var):\n",
    "    if var in dimension_map:\n",
    "        return dimension_map[var]\n",
    "    v = var.lower()\n",
    "    if any(k in v for k in [\"occupation\", \"work\", \"employment\", \"job\", \"hours\", \"basis\", \"industry\"]):\n",
    "        return \"Sensitivity\"\n",
    "    elif any(k in v for k in [\"grade\", \"school\", \"household\", \"age\", \"marital\", \"ethnicity\", \"training\"]):\n",
    "        return \"Resilience\"\n",
    "    elif any(k in v for k in [\"region\", \"province\", \"urban\", \"survey\", \"weight\", \"psu\", \"replicate\"]):\n",
    "        return \"Exposure\"\n",
    "    else:\n",
    "        return \"Unclassified\"\n",
    "\n",
    "decision_df[\"Dimension\"] = decision_df[\"Variable\"].apply(assign_dimension)\n",
    "\n",
    "# --- SuggestedAction logic ---\n",
    "def suggest_action(row):\n",
    "    fmi = row[\"OverallFMI\"]\n",
    "    tag = row[\"ConsistencyTag\"]\n",
    "\n",
    "    if pd.isna(fmi):\n",
    "        return \"review\"\n",
    "    if tag == \"consistent\":\n",
    "        if fmi < 0.05: return \"keep\"\n",
    "        elif fmi < 0.20: return \"impute_light\"\n",
    "        elif fmi < 0.40: return \"impute_cautious\"\n",
    "        else: return \"consider_drop_or_advanced\"\n",
    "    elif tag == \"partial\":\n",
    "        if fmi < 0.20: return \"sensitivity_only\"\n",
    "        else: return \"exclude_from_FA\"\n",
    "    else:  # inconsistent\n",
    "        return \"exclude_from_FA\"\n",
    "\n",
    "decision_df[\"Action\"] = decision_df.apply(suggest_action, axis=1)\n",
    "\n",
    "# --- Reorder columns for clarity ---\n",
    "decision_df = decision_df[[\n",
    "    \"Variable\", \"ConsistencyTag\", \"OverallFMI\", \"Flag\",\n",
    "    \"Dimension\", \"Action\", \n",
    "]]\n",
    "\n",
    "# --- Save template ---\n",
    "out_file = DECISION_ROOT / \"Decision_Matrix.csv\"\n",
    "decision_df.to_csv(out_file, index=False)\n",
    "print(f\"[OK] Decision matrix template saved to {out_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "510a30fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>ConsistencyTag</th>\n",
       "      <th>OverallFMI</th>\n",
       "      <th>Flag</th>\n",
       "      <th>Dimension</th>\n",
       "      <th>Action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Available for Work</td>\n",
       "      <td>consistent</td>\n",
       "      <td>0.962342</td>\n",
       "      <td>Critical</td>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>consider_drop_or_advanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C03-Relationship to Household Head</td>\n",
       "      <td>consistent</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Low</td>\n",
       "      <td>Resilience</td>\n",
       "      <td>keep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C04-Sex</td>\n",
       "      <td>consistent</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Low</td>\n",
       "      <td>Resilience</td>\n",
       "      <td>keep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C05-Age as of Last Birthday</td>\n",
       "      <td>consistent</td>\n",
       "      <td>0.020054</td>\n",
       "      <td>Low</td>\n",
       "      <td>Resilience</td>\n",
       "      <td>keep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C05B - Ethnicity</td>\n",
       "      <td>inconsistent</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Low</td>\n",
       "      <td>Resilience</td>\n",
       "      <td>exclude_from_FA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>C06-Marital Status</td>\n",
       "      <td>consistent</td>\n",
       "      <td>0.072348</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>Resilience</td>\n",
       "      <td>impute_light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>C07-Highest Grade Completed</td>\n",
       "      <td>consistent</td>\n",
       "      <td>0.072864</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>Resilience</td>\n",
       "      <td>impute_light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>C08-Currently Attending School</td>\n",
       "      <td>inconsistent</td>\n",
       "      <td>0.557455</td>\n",
       "      <td>Critical</td>\n",
       "      <td>Resilience</td>\n",
       "      <td>exclude_from_FA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>C08-Overseas Filipino Indicator</td>\n",
       "      <td>inconsistent</td>\n",
       "      <td>0.267195</td>\n",
       "      <td>High</td>\n",
       "      <td>Unclassified</td>\n",
       "      <td>exclude_from_FA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>C09-Graduate of technical/vocational course</td>\n",
       "      <td>inconsistent</td>\n",
       "      <td>0.281280</td>\n",
       "      <td>High</td>\n",
       "      <td>Resilience</td>\n",
       "      <td>exclude_from_FA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Variable ConsistencyTag  OverallFMI  \\\n",
       "0                           Available for Work     consistent    0.962342   \n",
       "1           C03-Relationship to Household Head     consistent    0.000000   \n",
       "2                                      C04-Sex     consistent    0.000000   \n",
       "3                  C05-Age as of Last Birthday     consistent    0.020054   \n",
       "4                             C05B - Ethnicity   inconsistent    0.000000   \n",
       "5                           C06-Marital Status     consistent    0.072348   \n",
       "6                  C07-Highest Grade Completed     consistent    0.072864   \n",
       "7               C08-Currently Attending School   inconsistent    0.557455   \n",
       "8              C08-Overseas Filipino Indicator   inconsistent    0.267195   \n",
       "9  C09-Graduate of technical/vocational course   inconsistent    0.281280   \n",
       "\n",
       "       Flag     Dimension                     Action  \n",
       "0  Critical   Sensitivity  consider_drop_or_advanced  \n",
       "1       Low    Resilience                       keep  \n",
       "2       Low    Resilience                       keep  \n",
       "3       Low    Resilience                       keep  \n",
       "4       Low    Resilience            exclude_from_FA  \n",
       "5  Moderate    Resilience               impute_light  \n",
       "6  Moderate    Resilience               impute_light  \n",
       "7  Critical    Resilience            exclude_from_FA  \n",
       "8      High  Unclassified            exclude_from_FA  \n",
       "9      High    Resilience            exclude_from_FA  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa83e5fe",
   "metadata": {},
   "source": [
    "#### CRUCIAL NOTES (README)\n",
    "\n",
    "-  Not sure with the difference between `work indicator and work indicator.1.` Kindly see Decision_Matrix sheets for granular details and `metadata sheet 1/2` for definitions.\n",
    "-  Also Check `Province and Province Recode` for missing values. Not sure what kind of imputation is applicable for this one since (assuming manual imputation, since lists of provinces can be acquired online and shall serve as a guide for encoding.). But we can still automate  this given that we have a strict list of dictionary once its acquired from online. IMPROPER IMPUTATION will done at this test stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255af06f",
   "metadata": {},
   "source": [
    "### Decision Matrix for Imputation - Defense Strategy\n",
    "\n",
    "This matrix is the bridge between our FMI diagnostics (Notebook 08) and the Factor Analysis (Notebook 11).\n",
    "It ensures that **every variable** is evaluated not just by numbers, but by its **role** in the thesis framework:\n",
    "\n",
    "- **Sensitivity**: Employment stability, income regularity.\n",
    "- **Resilience**: Education, skills, adaptability.\n",
    "- **Exposure**: Region, Urban/Rural context.\n",
    "\n",
    "#### Why automate?\n",
    "Manual grouping is prone to error. We encoded the rules into a dictionary so we can reproduce the results exactly if the panel asks us to run it again.\n",
    "- The `dimension_map` handles the assignments.\n",
    "- \"Unclassified\" variables are flagged so we don't miss anything.\n",
    "\n",
    "#### Why is this defensible?\n",
    "- **Theory-based**: Aligns with Voith & Mauser (2024).\n",
    "- **Smart Imputation**: We are **NOT** using Mode for everything. That creates bias. We are distinguishing between **MAR** (Stochastic) and **MNAR** (Drop).\n",
    "- **Audit-ready**: The code logs every single decision. We can show the panel exactly why a variable was dropped or imputed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b73f6b",
   "metadata": {},
   "source": [
    "### Imputation Proper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223895e3",
   "metadata": {},
   "source": [
    "At this stage, we apply the \"Smart\" imputation. We implemented a **Universal Stochastic** approach for categorical variables instead of Mode, even for MCAR. This preserves the natural distribution of the data (e.g., ratio of farmers to fishermen) rather than artificially inflating the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fa72926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Missingness Diagnostics.\n",
      "\n",
      "Scanning 21 consistent variables...\n",
      " > Dropping 0 MNAR variables globally.\n",
      " > Processing 21 variables for Smart Imputation.\n",
      "--------------------------------------------------\n",
      "Processing APRIL_2018.CSV...\n",
      "   Saved: imputed_APRIL_2018.csv\n",
      "Processing JULY_2018.CSV...\n",
      "   Saved: imputed_JULY_2018.csv\n",
      "Processing OCTOBER_2018.CSV...\n",
      "   Saved: imputed_OCTOBER_2018.csv\n",
      "Processing JANUARY_2018.CSV...\n",
      "   Saved: imputed_JANUARY_2018.csv\n",
      "Processing APRIL_2019.CSV...\n",
      "   Saved: imputed_APRIL_2019.csv\n",
      "Processing OCTOBER_2019.CSV...\n",
      "   Saved: imputed_OCTOBER_2019.csv\n",
      "Processing JANUARY_2019.CSV...\n",
      "   Saved: imputed_JANUARY_2019.csv\n",
      "Processing JULY_2019.CSV...\n",
      "   Saved: imputed_JULY_2019.csv\n",
      "Processing JUNE_2022.csv...\n",
      "   Saved: imputed_JUNE_2022.csv\n",
      "Processing FEBRUARY_2022.csv...\n",
      "   Saved: imputed_FEBRUARY_2022.csv\n",
      "Processing AUGUST_2022.CSV...\n",
      "   Saved: imputed_AUGUST_2022.csv\n",
      "Processing JULY_2022.CSV...\n",
      "   Saved: imputed_JULY_2022.csv\n",
      "Processing DECEMBER_2022.CSV...\n",
      "   Saved: imputed_DECEMBER_2022.csv\n",
      "Processing MARCH_2022.csv...\n",
      "   Saved: imputed_MARCH_2022.csv\n",
      "Processing NOVEMBER_2022.CSV...\n",
      "   Saved: imputed_NOVEMBER_2022.csv\n",
      "Processing MAY_2022.csv...\n",
      "   Saved: imputed_MAY_2022.csv\n",
      "Processing SEPTEMBER_2022.CSV...\n",
      "   Saved: imputed_SEPTEMBER_2022.csv\n",
      "Processing APRIL_2022.csv...\n",
      "   Saved: imputed_APRIL_2022.csv\n",
      "Processing OCTOBER_2022.CSV...\n",
      "   Saved: imputed_OCTOBER_2022.csv\n",
      "Processing JANUARY_2022.csv...\n",
      "   Saved: imputed_JANUARY_2022.csv\n",
      "Processing DECEMBER_2023.CSV...\n",
      "   Saved: imputed_DECEMBER_2023.csv\n",
      "Processing FEBRUARY_2023.CSV...\n",
      "   Saved: imputed_FEBRUARY_2023.csv\n",
      "Processing AUGUST_2023.CSV...\n",
      "   Saved: imputed_AUGUST_2023.csv\n",
      "Processing JUNE_2023.CSV...\n",
      "   Saved: imputed_JUNE_2023.csv\n",
      "Processing MARCH_2023.CSV...\n",
      "   Saved: imputed_MARCH_2023.csv\n",
      "Processing NOVEMBER_2023.CSV...\n",
      "   Saved: imputed_NOVEMBER_2023.csv\n",
      "Processing SEPTEMBER_2023.CSV...\n",
      "   Saved: imputed_SEPTEMBER_2023.csv\n",
      "Processing MAY_2023.CSV...\n",
      "   Saved: imputed_MAY_2023.csv\n",
      "Processing APRIL_2023.CSV...\n",
      "   Saved: imputed_APRIL_2023.csv\n",
      "Processing JANUARY_2023.CSV...\n",
      "   Saved: imputed_JANUARY_2023.csv\n",
      "Processing OCTOBER_2023.CSV...\n",
      "   Saved: imputed_OCTOBER_2023.csv\n",
      "Processing JULY_2023.CSV...\n",
      "   Saved: imputed_JULY_2023.csv\n",
      "Processing AUGUST_2024.CSV...\n",
      "   Saved: imputed_AUGUST_2024.csv\n",
      "Processing FEBRUARY_2024.CSV...\n",
      "   Saved: imputed_FEBRUARY_2024.csv\n",
      "Processing MARCH_2024.CSV...\n",
      "   Saved: imputed_MARCH_2024.csv\n",
      "Processing MAY_2024.CSV...\n",
      "   Saved: imputed_MAY_2024.csv\n",
      "Processing JUNE_2024.CSV...\n",
      "   Saved: imputed_JUNE_2024.csv\n",
      "Processing APRIL_2024.CSV...\n",
      "   Saved: imputed_APRIL_2024.csv\n",
      "Processing JULY_2024.CSV...\n",
      "   Saved: imputed_JULY_2024.csv\n",
      "Processing JANUARY_2024.CSV...\n",
      "   Saved: imputed_JANUARY_2024.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "from sklearn.impute import KNNImputer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from difflib import get_close_matches\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Configuration & Paths ---\n",
    "INPUT_ROOT = BASE_PATH / \"NEW Renamed Fully Decoded Surveys\"\n",
    "CONSISTENCY_ROOT = BASE_PATH / \"NEW Variable Consistency Check\"\n",
    "FMI_ROOT = BASE_PATH / \"NEW FMI Reports\"\n",
    "METADATA_ROOT = BASE_PATH / \"NEW Metadata Sheet 2 CSVs\"\n",
    "DIAGNOSTIC_ROOT = BASE_PATH / \"Missingness Reports\"  \n",
    "OUTPUT_ROOT = BASE_PATH / \"Imputed Data for Analysis\"\n",
    "\n",
    "OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Load Profiles ---\n",
    "consistency_df = pd.read_csv(CONSISTENCY_ROOT / \"consistency_profile.csv\")\n",
    "fmi_df = pd.read_csv(FMI_ROOT / \"fmi_profile.csv\")\n",
    "\n",
    "# Load Diagnostic Report (Notebook 09)\n",
    "try:\n",
    "    diagnostic_df = pd.read_csv(DIAGNOSTIC_ROOT / \"missingness_diagnostic_report.csv\")\n",
    "    diagnostic_map = dict(zip(diagnostic_df['variable'], diagnostic_df['verdict']))\n",
    "    print(\"Loaded Missingness Diagnostics.\")\n",
    "except:\n",
    "    print(\"Warning: Diagnostics not found. MNAR drop disabled.\")\n",
    "    diagnostic_map = {}\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def normalize_name(name: str) -> str:\n",
    "    return (str(name).strip().lower().replace(\"\\xa0\", \" \").replace(\"-\", \" \").replace(\"_\", \" \"))\n",
    "\n",
    "def find_column(df, var):\n",
    "    cols_norm = {normalize_name(c): c for c in df.columns}\n",
    "    var_norm = normalize_name(var)\n",
    "    if var_norm in cols_norm: return cols_norm[var_norm]\n",
    "    matches = get_close_matches(var_norm, list(cols_norm.keys()), n=1, cutoff=0.8)\n",
    "    return cols_norm[matches[0]] if matches else None\n",
    "\n",
    "def clean_age_column(col: pd.Series) -> pd.Series:\n",
    "    s = col.astype(str)\n",
    "    s = s.where(~s.str.contains(r\"\\d{4}-\\d{2}-\\d{2}\", regex=True), \"UnknownAge\")\n",
    "    numeric = pd.to_numeric(s, errors=\"coerce\")\n",
    "    if numeric.notna().sum() >= (0.5 * len(s)):\n",
    "        return numeric.fillna(-1).astype(int)\n",
    "    return s.replace({\"nan\": \"UnknownAge\"})\n",
    "\n",
    "# --- SMART IMPUTATION ENGINES ---\n",
    "\n",
    "def stochastic_impute(series):\n",
    "    # SMART IMPUTATION:\n",
    "    # Instead of Mode (which biases towards majority), sample from the probability distribution.\n",
    "    counts = series.value_counts(normalize=True)\n",
    "    missing = series.isna()\n",
    "    if missing.sum() == 0: return series\n",
    "    \n",
    "    # Roll the dice based on existing probabilities\n",
    "    fill_vals = np.random.choice(counts.index, size=missing.sum(), p=counts.values)\n",
    "    series.loc[missing] = fill_vals\n",
    "    return series\n",
    "\n",
    "def knn_impute(df, numeric_cols):\n",
    "    # Numeric Imputation: Use Euclidean distance to find similar respondents\n",
    "    if not numeric_cols or df[numeric_cols].isna().sum().sum() == 0: return df\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    df[numeric_cols] = imputer.fit_transform(df[numeric_cols])\n",
    "    return df\n",
    "\n",
    "# --- Pre-processing: Identify MNAR Variables ---\n",
    "all_vars = consistency_df[consistency_df[\"ConsistencyTag\"] == \"consistent\"][\"Variable\"].tolist()\n",
    "vars_to_drop = []\n",
    "vars_to_process = []\n",
    "\n",
    "print(f\"\\nScanning {len(all_vars)} consistent variables...\")\n",
    "\n",
    "for var in all_vars:\n",
    "    diag = diagnostic_map.get(normalize_name(var), \"Unknown\")\n",
    "    # Global Kill Switch for MNAR\n",
    "    if \"MNAR\" in diag:\n",
    "        vars_to_drop.append(var)\n",
    "    else:\n",
    "        vars_to_process.append(var)\n",
    "\n",
    "print(f\" > Dropping {len(vars_to_drop)} MNAR variables globally.\")\n",
    "print(f\" > Processing {len(vars_to_process)} variables for Smart Imputation.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- Main Loop ---\n",
    "\n",
    "for year_folder in INPUT_ROOT.iterdir():\n",
    "    if not year_folder.is_dir(): continue\n",
    "\n",
    "    year_out_dir = OUTPUT_ROOT / year_folder.name\n",
    "    year_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for file in year_folder.glob(\"*.csv\"):\n",
    "        print(f\"Processing {file.name}...\")\n",
    "        df = pd.read_csv(file, low_memory=False)\n",
    "        df.columns = [normalize_name(c) for c in df.columns]\n",
    "        \n",
    "        # Set Seed for Reproducibility\n",
    "        np.random.seed(42)\n",
    "\n",
    "        audit_log = []\n",
    "\n",
    "        # 1. Global Drop (MNAR)\n",
    "        for var in vars_to_drop:\n",
    "            col = find_column(df, var)\n",
    "            if col:\n",
    "                df.drop(columns=[col], inplace=True)\n",
    "                audit_log.append({\"Variable\": col, \"Action\": \"Dropped (MNAR)\", \"Note\": \"Diagnosed as MNAR\"})\n",
    "\n",
    "        # 2. Map target columns\n",
    "        targets = []\n",
    "        for var in vars_to_process:\n",
    "            col = find_column(df, var)\n",
    "            if col: targets.append((var, col))\n",
    "\n",
    "        # 3. Numeric Imputation (KNN)\n",
    "        numeric_cols = []\n",
    "        for var, col in targets:\n",
    "            if normalize_name(var) == \"c05 age as of last birthday\":\n",
    "                df[col] = clean_age_column(df[col])\n",
    "            \n",
    "            # Safety Valve (>50%)\n",
    "            if df[col].isna().mean() > 0.50:\n",
    "                audit_log.append({\"Variable\": col, \"Action\": \"Skipped\", \"Note\": \">50% Missing Data\"})\n",
    "                continue\n",
    "                \n",
    "            if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                numeric_cols.append(col)\n",
    "\n",
    "        if numeric_cols:\n",
    "            df = knn_impute(df, numeric_cols)\n",
    "            for c in numeric_cols:\n",
    "                if df[c].isna().sum() == 0:\n",
    "                    audit_log.append({\"Variable\": c, \"Action\": \"Imputed (KNN)\", \"Note\": \"Smart Numeric Fill\"})\n",
    "\n",
    "        # 4. Categorical Imputation (UNIVERSAL STOCHASTIC)\n",
    "        for var, col in targets:\n",
    "            if col in numeric_cols: continue\n",
    "            \n",
    "            # Normalize blanks\n",
    "            df[col] = df[col].replace(r'^\\s*$', np.nan, regex=True)\n",
    "            \n",
    "            # Safety Valve (>50%)\n",
    "            if df[col].isna().mean() > 0.50:\n",
    "                audit_log.append({\"Variable\": col, \"Action\": \"Skipped\", \"Note\": \">50% Missing Data\"})\n",
    "                continue\n",
    "            \n",
    "            if df[col].isna().sum() > 0:\n",
    "                # Use Stochastic for EVERYTHING categorical\n",
    "                df[col] = stochastic_impute(df[col])\n",
    "                \n",
    "                diag = diagnostic_map.get(normalize_name(var), \"Unknown\")\n",
    "                audit_log.append({\n",
    "                    \"Variable\": var, \n",
    "                    \"Action\": \"Imputed (Stochastic)\", \n",
    "                    \"Note\": f\"Preserved Distribution (Diag: {diag})\"\n",
    "                })\n",
    "\n",
    "        # Save Output\n",
    "        out_path = year_out_dir / f\"imputed_{file.stem}.csv\"\n",
    "        df.to_csv(out_path, index=False)\n",
    "        \n",
    "        if audit_log:\n",
    "            pd.DataFrame(audit_log).to_csv(year_out_dir / f\"log_{file.stem}.csv\", index=False)\n",
    "            \n",
    "        print(f\"   Saved: {out_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f2ca23",
   "metadata": {},
   "source": [
    "### Preprocessing and Imputation Summary\n",
    "\n",
    "**Column normalization**\n",
    "- All column names are standardized (lowercase, no spaces) so the Decision Matrix matches the survey files. \n",
    "\n",
    "**Global MNAR Filtering (The Kill Switch)**\n",
    "- We check the **Missingness Diagnostic Report** (Notebook 09) first.\n",
    "- If a variable is flagged as **Likely MNAR** (Missing Not at Random), we **auto-drop** it globally. This avoids introducing bias into the index.\n",
    "\n",
    "**The \"Safety Valve\"**\n",
    "- strict rule: **>50% missing = SKIP.**\n",
    "- If half the data is gone, imputing it makes it synthetic. We skip these to be safe.\n",
    "\n",
    "**Smart Imputation Logic (Why we did this)**\n",
    "- **Numeric**: Used **KNN (k=5)**. We need to preserve correlations (e.g., Age vs Hours Worked), not just fill blindly.\n",
    "- **Categorical**: We used **Universal Stochastic**.\n",
    "    - *Defense Point:* We intentionally avoided Mode Imputation. Mode inflates the majority class (e.g. making everyone a Farmer just because it's common). Stochastic sampling preserves the actual population variance.\n",
    "    - *Reproducibility:* Seed set to 42 so results don't change every run.\n",
    "\n",
    "**Audit logs**\n",
    "- Generated `imputation_log` files for every year. \n",
    "- Records exactly what happened (Imputed vs Dropped vs Skipped) so we can answer specific questions during the defense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f49880",
   "metadata": {},
   "source": [
    "### Evaluation of Imputation (By Completeness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f46e92d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Year                        File  TotalMissing Completeness\n",
      "0   2024       imputed_JUNE_2024.csv        704238         FAIL\n",
      "1   2024        imputed_MAY_2024.csv        716887         FAIL\n",
      "2   2024      imputed_MARCH_2024.csv        720972         FAIL\n",
      "3   2024       imputed_JULY_2024.csv       4140487         FAIL\n",
      "4   2024    imputed_JANUARY_2024.csv      17163849         FAIL\n",
      "5   2024     imputed_AUGUST_2024.csv        708376         FAIL\n",
      "6   2024      imputed_APRIL_2024.csv       4198712         FAIL\n",
      "7   2024   imputed_FEBRUARY_2024.csv        732372         FAIL\n",
      "8   2023        imputed_MAY_2023.csv        739436         FAIL\n",
      "9   2023  imputed_SEPTEMBER_2023.csv        736384         FAIL\n",
      "10  2023    imputed_OCTOBER_2023.csv       4391826         FAIL\n",
      "11  2023       imputed_JULY_2023.csv      17507521         FAIL\n",
      "12  2023      imputed_MARCH_2023.csv        752405         FAIL\n",
      "13  2023   imputed_NOVEMBER_2023.csv        730999         FAIL\n",
      "14  2023       imputed_JUNE_2023.csv        749647         FAIL\n",
      "15  2023    imputed_JANUARY_2023.csv       4408981         FAIL\n",
      "16  2023   imputed_FEBRUARY_2023.csv        764640         FAIL\n",
      "17  2023   imputed_DECEMBER_2023.csv        722031         FAIL\n",
      "18  2023     imputed_AUGUST_2023.csv        742414         FAIL\n",
      "19  2023      imputed_APRIL_2023.csv       4317282         FAIL\n",
      "20  2022  imputed_SEPTEMBER_2022.csv        758475         FAIL\n",
      "21  2022    imputed_OCTOBER_2022.csv       4407235         FAIL\n",
      "22  2022   imputed_NOVEMBER_2022.csv        735364         FAIL\n",
      "23  2022   imputed_DECEMBER_2022.csv        743915         FAIL\n",
      "24  2022     imputed_AUGUST_2022.csv        735261         FAIL\n",
      "25  2022       imputed_JULY_2022.csv       4401101         FAIL\n",
      "26  2022       imputed_JUNE_2022.csv        754881         FAIL\n",
      "27  2022   imputed_FEBRUARY_2022.csv        712697         FAIL\n",
      "28  2022      imputed_MARCH_2022.csv        715345         FAIL\n",
      "29  2022        imputed_MAY_2022.csv        764257         FAIL\n",
      "30  2022      imputed_APRIL_2022.csv       4455186         FAIL\n",
      "31  2022    imputed_JANUARY_2022.csv      17975676         FAIL\n",
      "32  2019    imputed_JANUARY_2019.csv       4004392         FAIL\n",
      "33  2019    imputed_OCTOBER_2019.csv       3889430         FAIL\n",
      "34  2019       imputed_JULY_2019.csv       3839339         FAIL\n",
      "35  2019      imputed_APRIL_2019.csv       3768242         FAIL\n",
      "36  2018    imputed_OCTOBER_2018.csv       3943117         FAIL\n",
      "37  2018    imputed_JANUARY_2018.csv       3934726         FAIL\n",
      "38  2018       imputed_JULY_2018.csv       4053455         FAIL\n",
      "39  2018      imputed_APRIL_2018.csv       3911509         FAIL\n",
      "\n",
      "Year-level completeness summary:\n",
      "Completeness  FAIL\n",
      "Year              \n",
      "2018             4\n",
      "2019             4\n",
      "2022            12\n",
      "2023            12\n",
      "2024             8\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "OUTPUT_ROOT = BASE_PATH / \"Imputed Data for Analysis\"\n",
    "\n",
    "summary_rows = []\n",
    "\n",
    "for year_folder in OUTPUT_ROOT.iterdir():\n",
    "    if not year_folder.is_dir():\n",
    "        continue\n",
    "\n",
    "    for file in year_folder.glob(\"imputed_*.csv\"):\n",
    "        df = pd.read_csv(file, low_memory=False)\n",
    "        null_counts = df.isnull().sum()\n",
    "        total_missing = int(null_counts.sum())\n",
    "\n",
    "        summary_rows.append({\n",
    "            \"Year\": year_folder.name,\n",
    "            \"File\": file.name,\n",
    "            \"TotalMissing\": total_missing,\n",
    "            \"Completeness\": \"PASS\" if total_missing == 0 else \"FAIL\",\n",
    "            **null_counts.to_dict()  # expand variable-level missing counts\n",
    "        })\n",
    "\n",
    "# Build DataFrame\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "\n",
    "# Preview file-level completeness\n",
    "print(summary_df[[\"Year\",\"File\",\"TotalMissing\",\"Completeness\"]])\n",
    "\n",
    "# Optional: Year-level summary\n",
    "year_summary = summary_df.groupby(\"Year\")[\"Completeness\"].value_counts().unstack(fill_value=0)\n",
    "print(\"\\nYear-level completeness summary:\")\n",
    "print(year_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f9db04",
   "metadata": {},
   "source": [
    "### Bias Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e8dd7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy.stats import ks_2samp, chi2_contingency\n",
    "\n",
    "# --- Paths ---\n",
    "RENAMED_ROOT = BASE_PATH / \"NEW Renamed Fully Decoded Surveys\"\n",
    "IMPUTED_ROOT = BASE_PATH / \"Imputed Data for Analysis\"\n",
    "CONSISTENCY_ROOT = BASE_PATH / \"NEW Variable Consistency Check\"\n",
    "\n",
    "# --- Load consistent variables ---\n",
    "consistency_df = pd.read_csv(CONSISTENCY_ROOT / \"consistency_profile.csv\")\n",
    "consistent_vars = consistency_df[consistency_df[\"ConsistencyTag\"] == \"consistent\"][\"Variable\"].tolist()\n",
    "\n",
    "# --- Normalization helper ---\n",
    "def normalize_name(name: str) -> str:\n",
    "    return (\n",
    "        str(name)\n",
    "        .strip()\n",
    "        .lower()\n",
    "        .replace(\"\\xa0\", \" \")\n",
    "        .replace(\"-\", \" \")\n",
    "        .replace(\"_\", \" \")\n",
    "    )\n",
    "\n",
    "consistent_vars_norm = [normalize_name(v) for v in consistent_vars]\n",
    "\n",
    "# --- Bias evaluation helpers ---\n",
    "def evaluate_numeric_bias(raw, imp):\n",
    "    \"\"\"Numeric bias check: KS test (distribution similarity) + RMSE (value closeness).\"\"\"\n",
    "    raw_clean = pd.to_numeric(raw, errors=\"coerce\").dropna()\n",
    "    imp_clean = pd.to_numeric(imp, errors=\"coerce\").dropna()\n",
    "    # Require at least 10 valid values on both sides\n",
    "    if len(raw_clean) < 10 or len(imp_clean) < 10:\n",
    "        return {\"KS_p\": np.nan, \"RMSE\": np.nan}\n",
    "    ks_stat, ks_p = ks_2samp(raw_clean, imp_clean)\n",
    "    # Align lengths safely\n",
    "    min_len = min(len(raw_clean), len(imp_clean))\n",
    "    rmse = np.sqrt(np.mean((raw_clean.values[:min_len] - imp_clean.values[:min_len])**2))\n",
    "    return {\"KS_p\": ks_p, \"RMSE\": rmse}\n",
    "\n",
    "def evaluate_categorical_bias(raw, imp):\n",
    "    \"\"\"Categorical bias check: Chi-square test comparing distributions.\"\"\"\n",
    "    raw_counts = raw.value_counts()\n",
    "    imp_counts = imp.value_counts()\n",
    "    all_cats = set(raw_counts.index).union(set(imp_counts.index))\n",
    "    raw_vec = [raw_counts.get(c,0) for c in all_cats]\n",
    "    imp_vec = [imp_counts.get(c,0) for c in all_cats]\n",
    "    try:\n",
    "        chi2, p, _, _ = chi2_contingency([raw_vec, imp_vec])\n",
    "    except:\n",
    "        p = np.nan\n",
    "    return {\"Chi2_p\": p}\n",
    "\n",
    "# --- Bias flagging logic ---\n",
    "def flag_bias(row):\n",
    "    if not pd.isna(row.get(\"Chi2_p\")):\n",
    "        return \"Potential Bias (categorical shift)\" if row[\"Chi2_p\"] < 0.05 else \"No Bias Detected\"\n",
    "    elif not pd.isna(row.get(\"KS_p\")):\n",
    "        return \"Potential Bias (numeric shift)\" if row[\"KS_p\"] < 0.05 or (not pd.isna(row.get(\"RMSE\")) and row[\"RMSE\"] > 0.5) else \"No Bias Detected\"\n",
    "    else:\n",
    "        return \"Not Evaluated\"\n",
    "\n",
    "# --- Documentation for metrics ---\n",
    "METRIC_DOC = {\n",
    "    \"Chi2_p\": \"Chi-square test p-value: <0.05 means categorical distribution changed after imputation.\",\n",
    "    \"KS_p\": \"Kolmogorov-Smirnov test p-value: <0.05 means numeric distribution changed after imputation.\",\n",
    "    \"RMSE\": \"Root Mean Squared Error: higher values mean imputed values deviate from observed distribution.\"\n",
    "}\n",
    "\n",
    "# --- Evaluation loop ---\n",
    "results = []\n",
    "\n",
    "for year in os.listdir(RENAMED_ROOT):\n",
    "    year_raw = RENAMED_ROOT / year\n",
    "    year_imp = IMPUTED_ROOT / year\n",
    "    if not year_raw.is_dir() or not year_imp.is_dir():\n",
    "        continue\n",
    "\n",
    "    for file in os.listdir(year_raw):\n",
    "        if not file.endswith(\".CSV\"):\n",
    "            continue\n",
    "        month = file.split(\"_\")[0].capitalize()\n",
    "\n",
    "        raw_df = pd.read_csv(year_raw / file, low_memory=False)\n",
    "        imp_df = pd.read_csv(year_imp / f\"imputed_{file}\", low_memory=False)\n",
    "\n",
    "        # Normalize column names\n",
    "        raw_df.columns = [normalize_name(c) for c in raw_df.columns]\n",
    "        imp_df.columns = [normalize_name(c) for c in imp_df.columns]\n",
    "\n",
    "        for var in consistent_vars_norm:\n",
    "            if var not in raw_df.columns or var not in imp_df.columns:\n",
    "                continue\n",
    "\n",
    "            raw_col = raw_df[var].dropna()\n",
    "            imp_col = imp_df[var].dropna()\n",
    "\n",
    "            if pd.api.types.is_numeric_dtype(raw_df[var]):\n",
    "                metrics = evaluate_numeric_bias(raw_col, imp_col)\n",
    "            else:\n",
    "                metrics = evaluate_categorical_bias(raw_col, imp_col)\n",
    "\n",
    "            row = {\"Year\": year, \"Month\": month, \"Variable\": var, **metrics}\n",
    "            row[\"BiasFlag\"] = flag_bias(row)\n",
    "            row[\"Chi2_p_doc\"] = METRIC_DOC[\"Chi2_p\"]\n",
    "            row[\"KS_p_doc\"] = METRIC_DOC[\"KS_p\"]\n",
    "            row[\"RMSE_doc\"] = METRIC_DOC[\"RMSE\"]\n",
    "            results.append(row)\n",
    "\n",
    "# --- Build DataFrame ---\n",
    "eval_df = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "517a3658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Variable</th>\n",
       "      <th>Chi2_p</th>\n",
       "      <th>BiasFlag</th>\n",
       "      <th>Chi2_p_doc</th>\n",
       "      <th>KS_p_doc</th>\n",
       "      <th>RMSE_doc</th>\n",
       "      <th>KS_p</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018</td>\n",
       "      <td>April</td>\n",
       "      <td>available for work</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>No Bias Detected</td>\n",
       "      <td>Chi-square test p-value: &lt;0.05 means categoric...</td>\n",
       "      <td>Kolmogorov-Smirnov test p-value: &lt;0.05 means n...</td>\n",
       "      <td>Root Mean Squared Error: higher values mean im...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018</td>\n",
       "      <td>April</td>\n",
       "      <td>c03 relationship to household head</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>No Bias Detected</td>\n",
       "      <td>Chi-square test p-value: &lt;0.05 means categoric...</td>\n",
       "      <td>Kolmogorov-Smirnov test p-value: &lt;0.05 means n...</td>\n",
       "      <td>Root Mean Squared Error: higher values mean im...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018</td>\n",
       "      <td>April</td>\n",
       "      <td>c04 sex</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>No Bias Detected</td>\n",
       "      <td>Chi-square test p-value: &lt;0.05 means categoric...</td>\n",
       "      <td>Kolmogorov-Smirnov test p-value: &lt;0.05 means n...</td>\n",
       "      <td>Root Mean Squared Error: higher values mean im...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018</td>\n",
       "      <td>April</td>\n",
       "      <td>c05 age as of last birthday</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No Bias Detected</td>\n",
       "      <td>Chi-square test p-value: &lt;0.05 means categoric...</td>\n",
       "      <td>Kolmogorov-Smirnov test p-value: &lt;0.05 means n...</td>\n",
       "      <td>Root Mean Squared Error: higher values mean im...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018</td>\n",
       "      <td>April</td>\n",
       "      <td>c06 marital status</td>\n",
       "      <td>0.999445</td>\n",
       "      <td>No Bias Detected</td>\n",
       "      <td>Chi-square test p-value: &lt;0.05 means categoric...</td>\n",
       "      <td>Kolmogorov-Smirnov test p-value: &lt;0.05 means n...</td>\n",
       "      <td>Root Mean Squared Error: higher values mean im...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018</td>\n",
       "      <td>April</td>\n",
       "      <td>c07 highest grade completed</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>No Bias Detected</td>\n",
       "      <td>Chi-square test p-value: &lt;0.05 means categoric...</td>\n",
       "      <td>Kolmogorov-Smirnov test p-value: &lt;0.05 means n...</td>\n",
       "      <td>Root Mean Squared Error: higher values mean im...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018</td>\n",
       "      <td>April</td>\n",
       "      <td>c101 line number</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No Bias Detected</td>\n",
       "      <td>Chi-square test p-value: &lt;0.05 means categoric...</td>\n",
       "      <td>Kolmogorov-Smirnov test p-value: &lt;0.05 means n...</td>\n",
       "      <td>Root Mean Squared Error: higher values mean im...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018</td>\n",
       "      <td>April</td>\n",
       "      <td>household size</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No Bias Detected</td>\n",
       "      <td>Chi-square test p-value: &lt;0.05 means categoric...</td>\n",
       "      <td>Kolmogorov-Smirnov test p-value: &lt;0.05 means n...</td>\n",
       "      <td>Root Mean Squared Error: higher values mean im...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018</td>\n",
       "      <td>April</td>\n",
       "      <td>look for additional work</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>No Bias Detected</td>\n",
       "      <td>Chi-square test p-value: &lt;0.05 means categoric...</td>\n",
       "      <td>Kolmogorov-Smirnov test p-value: &lt;0.05 means n...</td>\n",
       "      <td>Root Mean Squared Error: higher values mean im...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018</td>\n",
       "      <td>April</td>\n",
       "      <td>looked for work or tried to establish business...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>No Bias Detected</td>\n",
       "      <td>Chi-square test p-value: &lt;0.05 means categoric...</td>\n",
       "      <td>Kolmogorov-Smirnov test p-value: &lt;0.05 means n...</td>\n",
       "      <td>Root Mean Squared Error: higher values mean im...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2018</td>\n",
       "      <td>April</td>\n",
       "      <td>new employment criteria (jul 05, 2005)</td>\n",
       "      <td>0.677735</td>\n",
       "      <td>No Bias Detected</td>\n",
       "      <td>Chi-square test p-value: &lt;0.05 means categoric...</td>\n",
       "      <td>Kolmogorov-Smirnov test p-value: &lt;0.05 means n...</td>\n",
       "      <td>Root Mean Squared Error: higher values mean im...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2018</td>\n",
       "      <td>April</td>\n",
       "      <td>normal working hours per day</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No Bias Detected</td>\n",
       "      <td>Chi-square test p-value: &lt;0.05 means categoric...</td>\n",
       "      <td>Kolmogorov-Smirnov test p-value: &lt;0.05 means n...</td>\n",
       "      <td>Root Mean Squared Error: higher values mean im...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2018</td>\n",
       "      <td>April</td>\n",
       "      <td>other job indicator</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>No Bias Detected</td>\n",
       "      <td>Chi-square test p-value: &lt;0.05 means categoric...</td>\n",
       "      <td>Kolmogorov-Smirnov test p-value: &lt;0.05 means n...</td>\n",
       "      <td>Root Mean Squared Error: higher values mean im...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2018</td>\n",
       "      <td>April</td>\n",
       "      <td>previous job indicator</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>No Bias Detected</td>\n",
       "      <td>Chi-square test p-value: &lt;0.05 means categoric...</td>\n",
       "      <td>Kolmogorov-Smirnov test p-value: &lt;0.05 means n...</td>\n",
       "      <td>Root Mean Squared Error: higher values mean im...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2018</td>\n",
       "      <td>April</td>\n",
       "      <td>psu number</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No Bias Detected</td>\n",
       "      <td>Chi-square test p-value: &lt;0.05 means categoric...</td>\n",
       "      <td>Kolmogorov-Smirnov test p-value: &lt;0.05 means n...</td>\n",
       "      <td>Root Mean Squared Error: higher values mean im...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2018</td>\n",
       "      <td>April</td>\n",
       "      <td>replicate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No Bias Detected</td>\n",
       "      <td>Chi-square test p-value: &lt;0.05 means categoric...</td>\n",
       "      <td>Kolmogorov-Smirnov test p-value: &lt;0.05 means n...</td>\n",
       "      <td>Root Mean Squared Error: higher values mean im...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2018</td>\n",
       "      <td>April</td>\n",
       "      <td>survey month</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>No Bias Detected</td>\n",
       "      <td>Chi-square test p-value: &lt;0.05 means categoric...</td>\n",
       "      <td>Kolmogorov-Smirnov test p-value: &lt;0.05 means n...</td>\n",
       "      <td>Root Mean Squared Error: higher values mean im...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2018</td>\n",
       "      <td>April</td>\n",
       "      <td>survey year</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No Bias Detected</td>\n",
       "      <td>Chi-square test p-value: &lt;0.05 means categoric...</td>\n",
       "      <td>Kolmogorov-Smirnov test p-value: &lt;0.05 means n...</td>\n",
       "      <td>Root Mean Squared Error: higher values mean im...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018</td>\n",
       "      <td>April</td>\n",
       "      <td>total hours worked for all jobs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No Bias Detected</td>\n",
       "      <td>Chi-square test p-value: &lt;0.05 means categoric...</td>\n",
       "      <td>Kolmogorov-Smirnov test p-value: &lt;0.05 means n...</td>\n",
       "      <td>Root Mean Squared Error: higher values mean im...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2018</td>\n",
       "      <td>April</td>\n",
       "      <td>want more hours of work</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>No Bias Detected</td>\n",
       "      <td>Chi-square test p-value: &lt;0.05 means categoric...</td>\n",
       "      <td>Kolmogorov-Smirnov test p-value: &lt;0.05 means n...</td>\n",
       "      <td>Root Mean Squared Error: higher values mean im...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Year  Month                                           Variable    Chi2_p  \\\n",
       "0   2018  April                                 available for work  1.000000   \n",
       "1   2018  April                 c03 relationship to household head  1.000000   \n",
       "2   2018  April                                            c04 sex  1.000000   \n",
       "3   2018  April                        c05 age as of last birthday       NaN   \n",
       "4   2018  April                                 c06 marital status  0.999445   \n",
       "5   2018  April                        c07 highest grade completed  1.000000   \n",
       "6   2018  April                                   c101 line number       NaN   \n",
       "7   2018  April                                     household size       NaN   \n",
       "8   2018  April                           look for additional work  1.000000   \n",
       "9   2018  April  looked for work or tried to establish business...  1.000000   \n",
       "10  2018  April             new employment criteria (jul 05, 2005)  0.677735   \n",
       "11  2018  April                       normal working hours per day       NaN   \n",
       "12  2018  April                                other job indicator  1.000000   \n",
       "13  2018  April                             previous job indicator  1.000000   \n",
       "14  2018  April                                         psu number       NaN   \n",
       "15  2018  April                                          replicate       NaN   \n",
       "16  2018  April                                       survey month  1.000000   \n",
       "17  2018  April                                        survey year       NaN   \n",
       "18  2018  April                    total hours worked for all jobs       NaN   \n",
       "19  2018  April                            want more hours of work  1.000000   \n",
       "\n",
       "            BiasFlag                                         Chi2_p_doc  \\\n",
       "0   No Bias Detected  Chi-square test p-value: <0.05 means categoric...   \n",
       "1   No Bias Detected  Chi-square test p-value: <0.05 means categoric...   \n",
       "2   No Bias Detected  Chi-square test p-value: <0.05 means categoric...   \n",
       "3   No Bias Detected  Chi-square test p-value: <0.05 means categoric...   \n",
       "4   No Bias Detected  Chi-square test p-value: <0.05 means categoric...   \n",
       "5   No Bias Detected  Chi-square test p-value: <0.05 means categoric...   \n",
       "6   No Bias Detected  Chi-square test p-value: <0.05 means categoric...   \n",
       "7   No Bias Detected  Chi-square test p-value: <0.05 means categoric...   \n",
       "8   No Bias Detected  Chi-square test p-value: <0.05 means categoric...   \n",
       "9   No Bias Detected  Chi-square test p-value: <0.05 means categoric...   \n",
       "10  No Bias Detected  Chi-square test p-value: <0.05 means categoric...   \n",
       "11  No Bias Detected  Chi-square test p-value: <0.05 means categoric...   \n",
       "12  No Bias Detected  Chi-square test p-value: <0.05 means categoric...   \n",
       "13  No Bias Detected  Chi-square test p-value: <0.05 means categoric...   \n",
       "14  No Bias Detected  Chi-square test p-value: <0.05 means categoric...   \n",
       "15  No Bias Detected  Chi-square test p-value: <0.05 means categoric...   \n",
       "16  No Bias Detected  Chi-square test p-value: <0.05 means categoric...   \n",
       "17  No Bias Detected  Chi-square test p-value: <0.05 means categoric...   \n",
       "18  No Bias Detected  Chi-square test p-value: <0.05 means categoric...   \n",
       "19  No Bias Detected  Chi-square test p-value: <0.05 means categoric...   \n",
       "\n",
       "                                             KS_p_doc  \\\n",
       "0   Kolmogorov-Smirnov test p-value: <0.05 means n...   \n",
       "1   Kolmogorov-Smirnov test p-value: <0.05 means n...   \n",
       "2   Kolmogorov-Smirnov test p-value: <0.05 means n...   \n",
       "3   Kolmogorov-Smirnov test p-value: <0.05 means n...   \n",
       "4   Kolmogorov-Smirnov test p-value: <0.05 means n...   \n",
       "5   Kolmogorov-Smirnov test p-value: <0.05 means n...   \n",
       "6   Kolmogorov-Smirnov test p-value: <0.05 means n...   \n",
       "7   Kolmogorov-Smirnov test p-value: <0.05 means n...   \n",
       "8   Kolmogorov-Smirnov test p-value: <0.05 means n...   \n",
       "9   Kolmogorov-Smirnov test p-value: <0.05 means n...   \n",
       "10  Kolmogorov-Smirnov test p-value: <0.05 means n...   \n",
       "11  Kolmogorov-Smirnov test p-value: <0.05 means n...   \n",
       "12  Kolmogorov-Smirnov test p-value: <0.05 means n...   \n",
       "13  Kolmogorov-Smirnov test p-value: <0.05 means n...   \n",
       "14  Kolmogorov-Smirnov test p-value: <0.05 means n...   \n",
       "15  Kolmogorov-Smirnov test p-value: <0.05 means n...   \n",
       "16  Kolmogorov-Smirnov test p-value: <0.05 means n...   \n",
       "17  Kolmogorov-Smirnov test p-value: <0.05 means n...   \n",
       "18  Kolmogorov-Smirnov test p-value: <0.05 means n...   \n",
       "19  Kolmogorov-Smirnov test p-value: <0.05 means n...   \n",
       "\n",
       "                                             RMSE_doc  KS_p  RMSE  \n",
       "0   Root Mean Squared Error: higher values mean im...   NaN   NaN  \n",
       "1   Root Mean Squared Error: higher values mean im...   NaN   NaN  \n",
       "2   Root Mean Squared Error: higher values mean im...   NaN   NaN  \n",
       "3   Root Mean Squared Error: higher values mean im...   1.0   0.0  \n",
       "4   Root Mean Squared Error: higher values mean im...   NaN   NaN  \n",
       "5   Root Mean Squared Error: higher values mean im...   NaN   NaN  \n",
       "6   Root Mean Squared Error: higher values mean im...   1.0   0.0  \n",
       "7   Root Mean Squared Error: higher values mean im...   1.0   0.0  \n",
       "8   Root Mean Squared Error: higher values mean im...   NaN   NaN  \n",
       "9   Root Mean Squared Error: higher values mean im...   NaN   NaN  \n",
       "10  Root Mean Squared Error: higher values mean im...   NaN   NaN  \n",
       "11  Root Mean Squared Error: higher values mean im...   1.0   0.0  \n",
       "12  Root Mean Squared Error: higher values mean im...   NaN   NaN  \n",
       "13  Root Mean Squared Error: higher values mean im...   NaN   NaN  \n",
       "14  Root Mean Squared Error: higher values mean im...   1.0   0.0  \n",
       "15  Root Mean Squared Error: higher values mean im...   1.0   0.0  \n",
       "16  Root Mean Squared Error: higher values mean im...   NaN   NaN  \n",
       "17  Root Mean Squared Error: higher values mean im...   1.0   0.0  \n",
       "18  Root Mean Squared Error: higher values mean im...   1.0   0.0  \n",
       "19  Root Mean Squared Error: higher values mean im...   NaN   NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6943aebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Overall Bias Summary =====\n",
      "Avg_RMSE: 511.05598959230406\n",
      "Max_RMSE: 28461.93365708258\n",
      "Avg_KS_p: 0.9781818181818182\n",
      "Avg_Chi2_p: 0.9636187273883147\n",
      "No_Bias_Count: 704\n",
      "Potential_Bias_Count: 10\n",
      "Not_Evaluated_Count: 0\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Overall Bias Summary Metrics\n",
    "# ============================================================\n",
    "\n",
    "summary = {}\n",
    "\n",
    "# Numeric variables\n",
    "numeric_df = eval_df.dropna(subset=[\"RMSE\"])\n",
    "summary[\"Avg_RMSE\"] = numeric_df[\"RMSE\"].mean()\n",
    "summary[\"Max_RMSE\"] = numeric_df[\"RMSE\"].max()\n",
    "summary[\"Avg_KS_p\"] = numeric_df[\"KS_p\"].mean()\n",
    "\n",
    "# Categorical variables\n",
    "categorical_df = eval_df.dropna(subset=[\"Chi2_p\"])\n",
    "summary[\"Avg_Chi2_p\"] = categorical_df[\"Chi2_p\"].mean()\n",
    "\n",
    "# Bias flag counts\n",
    "summary[\"No_Bias_Count\"] = (eval_df[\"BiasFlag\"] == \"No Bias Detected\").sum()\n",
    "summary[\"Potential_Bias_Count\"] = (eval_df[\"BiasFlag\"].str.contains(\"Potential Bias\")).sum()\n",
    "summary[\"Not_Evaluated_Count\"] = (eval_df[\"BiasFlag\"] == \"Not Evaluated\").sum()\n",
    "\n",
    "print(\"\\n===== Overall Bias Summary =====\")\n",
    "for k,v in summary.items():\n",
    "    print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd810d9",
   "metadata": {},
   "source": [
    "### **Limitations and Recommendations for Future Work**\n",
    "\n",
    "Our data cleaning strategy was largely successful. Out of **714 variables**, **704 (98.6%)** were validated as accurate with no significant errors. The average accuracy score was very high (0.978 out of 1.0).\n",
    "\n",
    "However, **10 variables** were flagged for potential errors, and our error metrics show extreme outliers (Maximum Error: **28,461.93**). Below are the specific limitations that caused these errors and direct recommendations for future researchers.\n",
    "\n",
    "#### **1. Distinguishing \"Not Applicable\" from \"Missing Data\"**\n",
    "* **The Issue:** Our system treated every blank cell as \"missing data\" that needed to be filled. In labor surveys, many cells are blank on purpose (e.g., the \"Secondary Job Salary\" question is blank because the person does not have a second job).\n",
    "* **The Result:** The algorithm likely calculated salaries for people who are unemployed or have no second job. This explains the massive Maximum Error (**28,461.93**). The model created values where there should have been zero.\n",
    "* **Recommendation:** Future work must add a \"Logic Check\" before filling data. If a person answers \"No\" to having a job, the algorithm should automatically set their salary to 0 instead of trying to guess a number. This would likely fix the 10 flagged variables.\n",
    "\n",
    "#### **2. Computing Power Constraints**\n",
    "* **The Issue:** Because the dataset covers six years (2018–2024) and is very large, we used a faster, single-pass method to fill missing values.\n",
    "* **The Result:** While accurate for the overall picture (Average Error: **511.06**), this method does not calculate the \"margin of error\" for each specific guess.\n",
    "* **Recommendation:** Researchers with access to supercomputers or high-performance clusters should use a method called **Multiple Imputation (MICE)**. This method runs the calculation multiple times to provide a range of probable values, giving a clearer picture of uncertainty.\n",
    "\n",
    "#### **3. Improving Guesses for Categories**\n",
    "* **The Issue:** For text-based answers (like \"Occupation\" or \"Province\"), we used a random sampling method based on probability. This keeps the total percentages correct but does not check if the guess makes sense for that specific person (e.g., checking if the Education level matches the Occupation).\n",
    "* **Recommendation:** Use Machine Learning models (like **XGBoost**) that can \"learn\" patterns. These models can look at a person's education and age to make a highly specific prediction for their missing Occupation, rather than just picking based on general probability.\n",
    "\n",
    "#### **4. Optimizing the \"Neighbor\" Count**\n",
    "* **The Issue:** When guessing a missing number for a person, our algorithm looked at the **5 most similar people** (k=5) in the dataset to calculate an average. We used 5 because it is the standard default.\n",
    "* **Recommendation:** Future studies should test different numbers (like 3, 10, or 20). Finding the perfect number of \"similar people\" to compare against could lower the Average Error (currently **511.06**), especially for variable numbers like Annual Income.\n",
    "\n",
    "#### **5. Adjusting the Drop Rule**\n",
    "* **The Issue:** We automatically deleted any survey question if more than **50%** of the answers were missing.\n",
    "* **Recommendation:** Some financial questions are critical but frequently skipped by respondents. Future work could allow a higher limit (e.g., keeping questions with 60% missing data) if researchers verify the filled data manually. This ensures that important financial details are not thrown away just because many people left them blank."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
